{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a7pfI9K2Ca7"
      },
      "source": [
        "# **8장 – 오토인코더, GAN 그리고 확산 모델**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFXIv9qNpKzt",
        "tags": []
      },
      "source": [
        "# 설정"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IPbJEmZpKzu"
      },
      "source": [
        "이 프로젝트에는 Python 3.7 이상이 필요합니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TFSU3FCOpKzu"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "assert sys.version_info >= (3, 7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAlKky09pKzv"
      },
      "source": [
        "또한 Scikit-Learn ≥ 1.0.1이 필요합니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqCwW7cMpKzw"
      },
      "outputs": [],
      "source": [
        "from packaging import version\n",
        "import sklearn\n",
        "\n",
        "assert version.parse(sklearn.__version__) >= version.parse(\"1.0.1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJtVEqxfpKzw"
      },
      "source": [
        "그리고 TensorFlow ≥ 2.8:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Piq5se2pKzx"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "assert version.parse(tf.__version__) >= version.parse(\"2.8.0\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDaDoLQTpKzx"
      },
      "source": [
        "이전 챕터에서 했던 것처럼 기본 글꼴 크기를 정의하여 그림을 더 예쁘게 만들어 보겠습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8d4TH3NbpKzx"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.rc('font', size=14)\n",
        "plt.rc('axes', labelsize=14, titlesize=14)\n",
        "plt.rc('legend', fontsize=14)\n",
        "plt.rc('xtick', labelsize=10)\n",
        "plt.rc('ytick', labelsize=10)\n",
        "\n",
        "import sys\n",
        "# 코랩의 경우 나눔 폰트를 설치합니다.\n",
        "if 'google.colab' in sys.modules:\n",
        "    !sudo apt-get -qq -y install fonts-nanum\n",
        "    import matplotlib.font_manager as fm\n",
        "    font_files = fm.findSystemFonts(fontpaths=['/usr/share/fonts/truetype/nanum'])\n",
        "    for fpath in font_files:\n",
        "        fm.fontManager.addfont(fpath)\n",
        "\n",
        "# 나눔 폰트를 사용합니다.\n",
        "import matplotlib\n",
        "\n",
        "matplotlib.rc('font', family='NanumBarunGothic')\n",
        "matplotlib.rcParams['axes.unicode_minus'] = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcoUIRsvpKzy"
      },
      "source": [
        "그리고 `images/generative` 폴더를 만들고(아직 존재하지 않는 경우), 이 노트북을 통해 책에 사용할 그림을 고해상도로 저장하는 데 사용되는 `save_fig()` 함수를 정의해 보겠습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQFH5Y9PpKzy"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "IMAGES_PATH = Path() / \"images\" / \"generative\"\n",
        "IMAGES_PATH.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
        "    path = IMAGES_PATH / f\"{fig_id}.{fig_extension}\"\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    plt.savefig(path, format=fig_extension, dpi=resolution)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTsawKlapKzy"
      },
      "source": [
        "이 챕터는 GPU가 없으면 매우 느려질 수 있으므로 GPU가 있는지 확인하거나 그렇지 않으면 경고를 표시합니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ekxzo6pOpKzy"
      },
      "outputs": [],
      "source": [
        "if not tf.config.list_physical_devices('GPU'):\n",
        "    print(\"GPU가 감지되지 않았습니다. 신경망은 GPU가 없으면 매우 느릴 수 있습니다.\")\n",
        "    if \"google.colab\" in sys.modules:\n",
        "        print(\"런타임 > 런타임 유형 변경으로 이동하여 하드웨어 가속기로 GPU를 선택합니다.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_JZF15b2CbI"
      },
      "source": [
        "# 과소완전 선형 오토인코더로 PCA 수행하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I88fkLUM2CbI"
      },
      "source": [
        "오토인코더를 만들어 보죠."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9iVri4j2CbI"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "tf.random.set_seed(42)  # 추가 코드 - CPU에서 재현성 보장\n",
        "\n",
        "encoder = tf.keras.Sequential([tf.keras.layers.Dense(2)])\n",
        "decoder = tf.keras.Sequential([tf.keras.layers.Dense(3)])\n",
        "autoencoder = tf.keras.Sequential([encoder, decoder])\n",
        "\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.5)\n",
        "autoencoder.compile(loss=\"mse\", optimizer=optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b16g_7Ak2CbJ"
      },
      "source": [
        "이제 8장에서 사용한 것과 동일한 3D 데이터셋을 생성해 보겠습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62woIglA2CbJ"
      },
      "outputs": [],
      "source": [
        "# 추가 코드\n",
        "\n",
        "import numpy as np\n",
        "from scipy.spatial.transform import Rotation\n",
        "\n",
        "m = 60\n",
        "X = np.zeros((m, 3))  # 3D 데이터 세트 초기화\n",
        "np.random.seed(42)\n",
        "angles = (np.random.rand(m) ** 3 + 0.5) * 2 * np.pi  # 고르지 않은 분포\n",
        "X[:, 0], X[:, 1] = np.cos(angles), np.sin(angles) * 0.5  # 타원형\n",
        "X += 0.28 * np.random.randn(m, 3)  # 노이즈 추가\n",
        "X = Rotation.from_rotvec([np.pi / 29, -np.pi / 20, np.pi / 4]).apply(X)\n",
        "X_train = X + [0.2, 0, 0.2]  # 약간 이동"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GdEKo0I72CbJ"
      },
      "outputs": [],
      "source": [
        "history = autoencoder.fit(X_train, X_train, epochs=500, verbose=False)\n",
        "codings = encoder.predict(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xnGvOZT62CbK"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(4,3))\n",
        "plt.plot(codings[:,0], codings[:, 1], \"b.\")\n",
        "plt.xlabel(\"$z_1$\", fontsize=18)\n",
        "plt.ylabel(\"$z_2$\", fontsize=18, rotation=0)\n",
        "plt.grid(True)\n",
        "save_fig(\"linear_autoencoder_pca_plot\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWIfp5lT2CbL"
      },
      "source": [
        "# 적층 오토인코더"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "SsmkbRuW2CbM"
      },
      "source": [
        "## 케라스를 사용하여 적층 오토인코더 구현하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfTWdMW42CbM"
      },
      "source": [
        "패션 MNIST 데이터셋을 로드하고 크기를 조정한 다음 훈련 세트, 검증 세트, 테스트 세트로 분할해 보겠습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89HwNxJJ2CbM"
      },
      "outputs": [],
      "source": [
        "# 추가 코드 - 패션 MNIST 데이터셋 로드, 스케일링 및 분할\n",
        "fashion_mnist = tf.keras.datasets.fashion_mnist.load_data()\n",
        "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist\n",
        "X_train_full = X_train_full.astype(np.float32) / 255\n",
        "X_test = X_test.astype(np.float32) / 255\n",
        "X_train, X_valid = X_train_full[:-5000], X_train_full[-5000:]\n",
        "y_train, y_valid = y_train_full[:-5000], y_train_full[-5000:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ggvpqWy2CbM"
      },
      "source": [
        "3개의 은닉 층과 1개의 출력 층으로 구성된 적층 오토인코더를 만들고 훈련해 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fMOjmZXg2CbN"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)  # 추가 코드 - CPU에서 재현성 보장\n",
        "\n",
        "stacked_encoder = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(30, activation=\"relu\"),\n",
        "])\n",
        "stacked_decoder = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(28 * 28),\n",
        "    tf.keras.layers.Reshape([28, 28])\n",
        "])\n",
        "stacked_ae = tf.keras.Sequential([stacked_encoder, stacked_decoder])\n",
        "\n",
        "stacked_ae.compile(loss=\"mse\", optimizer=\"nadam\")\n",
        "history = stacked_ae.fit(X_train, X_train, epochs=20,\n",
        "                         validation_data=(X_valid, X_valid))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Edq73sa5yLhC",
        "tags": []
      },
      "source": [
        "## 재구성 시각화하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yczyAq4x2CbN"
      },
      "source": [
        "이 함수는 몇 가지 검증 이미지를 오토인코더에 통과시키고 원본 이미지와 재구성된 이미지를 표시합니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZM-7U9XQ2CbN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def plot_reconstructions(model, images=X_valid, n_images=5):\n",
        "    reconstructions = np.clip(model.predict(images[:n_images]), 0, 1)\n",
        "    fig = plt.figure(figsize=(n_images * 1.5, 3))\n",
        "    for image_index in range(n_images):\n",
        "        plt.subplot(2, n_images, 1 + image_index)\n",
        "        plt.imshow(images[image_index], cmap=\"binary\")\n",
        "        plt.axis(\"off\")\n",
        "        plt.subplot(2, n_images, 1 + n_images + image_index)\n",
        "        plt.imshow(reconstructions[image_index], cmap=\"binary\")\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "plot_reconstructions(stacked_ae)\n",
        "save_fig(\"reconstruction_plot\")  # 추가 코드 - 고해상도 그림 저장\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzJSvIi82CbO"
      },
      "source": [
        "재구성된 이미지가 흐릿해 보이지만 784개가 아닌 30개의 숫자로 압축되었다는 점을 기억하세요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "c64CpPXw2CbO"
      },
      "source": [
        "## 패션 MNIST 데이터셋 시각화하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g69nLh2GyLhC"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "X_valid_compressed = stacked_encoder.predict(X_valid)\n",
        "tsne = TSNE(init=\"pca\", learning_rate=\"auto\", random_state=42)\n",
        "X_valid_2D = tsne.fit_transform(X_valid_compressed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "klppwl4gyLhC"
      },
      "outputs": [],
      "source": [
        "plt.scatter(X_valid_2D[:, 0], X_valid_2D[:, 1], c=y_valid, s=10, cmap=\"tab10\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xs95C-AtyLhC"
      },
      "source": [
        "이 다이어그램을 좀 더 예쁘게 만들어 봅시다([이 Scikit-Learn 예제](https://scikit-learn.org/stable/auto_examples/manifold/plot_lle_digits.html)를 참고했습니다):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JIkuJSpPyLhC"
      },
      "outputs": [],
      "source": [
        "# 추가 코드 - 이전 그래프를 아름답게 꾸밉니다.\n",
        "\n",
        "import matplotlib as mpl\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "cmap = plt.cm.tab10\n",
        "Z = X_valid_2D\n",
        "Z = (Z - Z.min()) / (Z.max() - Z.min())  # 0-1 범위로 정규화\n",
        "plt.scatter(Z[:, 0], Z[:, 1], c=y_valid, s=10, cmap=cmap)\n",
        "image_positions = np.array([[1., 1.]])\n",
        "for index, position in enumerate(Z):\n",
        "    dist = ((position - image_positions) ** 2).sum(axis=1)\n",
        "    if dist.min() > 0.02: # 다른 이미지와 충분히 멀리 떨어져 있는 경우\n",
        "        image_positions = np.r_[image_positions, [position]]\n",
        "        imagebox = mpl.offsetbox.AnnotationBbox(\n",
        "            mpl.offsetbox.OffsetImage(X_valid[index], cmap=\"binary\"),\n",
        "            position, bboxprops={\"edgecolor\": cmap(y_valid[index]), \"lw\": 2})\n",
        "        plt.gca().add_artist(imagebox)\n",
        "\n",
        "plt.axis(\"off\")\n",
        "save_fig(\"fashion_mnist_visualization_plot\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OJPbzoNyLhD"
      },
      "source": [
        "## 가중치 묶기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLMREMfUyLhD"
      },
      "source": [
        "인코더 가중치의 전치를 디코더 가중치로 사용하여 인코더와 디코더의 가중치를 묶는 것이 일반적입니다. 이를 위해서는 사용자 정의 층을 사용해야 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jTru0q0CyLhD"
      },
      "outputs": [],
      "source": [
        "class DenseTranspose(tf.keras.layers.Layer):\n",
        "    def __init__(self, dense, activation=None, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.dense = dense\n",
        "        self.activation = tf.keras.activations.get(activation)\n",
        "\n",
        "    def build(self, batch_input_shape):\n",
        "        self.biases = self.add_weight(name=\"bias\",\n",
        "                                      shape=self.dense.input_shape[-1],\n",
        "                                      initializer=\"zeros\")\n",
        "        super().build(batch_input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        Z = tf.matmul(inputs, self.dense.weights[0], transpose_b=True)\n",
        "        return self.activation(Z + self.biases)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_mU3e6IyLhD"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)  # 추가 코드 - CPU에서 재현성 보장\n",
        "\n",
        "dense_1 = tf.keras.layers.Dense(100, activation=\"relu\")\n",
        "dense_2 = tf.keras.layers.Dense(30, activation=\"relu\")\n",
        "\n",
        "tied_encoder = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(),\n",
        "    dense_1,\n",
        "    dense_2\n",
        "])\n",
        "\n",
        "tied_decoder = tf.keras.Sequential([\n",
        "    DenseTranspose(dense_2, activation=\"relu\"),\n",
        "    DenseTranspose(dense_1),\n",
        "    tf.keras.layers.Reshape([28, 28])\n",
        "])\n",
        "\n",
        "tied_ae = tf.keras.Sequential([tied_encoder, tied_decoder])\n",
        "\n",
        "# 추가 코드 - 모델을 컴파일하고 훈련합니다.\n",
        "tied_ae.compile(loss=\"mse\", optimizer=\"nadam\")\n",
        "history = tied_ae.fit(X_train, X_train, epochs=10,\n",
        "                      validation_data=(X_valid, X_valid))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WS2bm471yLhD",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# 추가 코드 - 플롯 재구성\n",
        "plot_reconstructions(tied_ae)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucUS-1GwyLhE"
      },
      "source": [
        "## 추가 자료 - 한 번에 하나씩 오토인코더 훈련하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CIqJ922pyLhE"
      },
      "outputs": [],
      "source": [
        "def train_autoencoder(n_neurons, X_train, X_valid, n_epochs=10,\n",
        "                      output_activation=None):\n",
        "    n_inputs = X_train.shape[-1]\n",
        "    encoder = tf.keras.layers.Dense(n_neurons, activation=\"relu\")\n",
        "    decoder = tf.keras.layers.Dense(n_inputs, activation=output_activation)\n",
        "    autoencoder = tf.keras.Sequential([encoder, decoder])\n",
        "    autoencoder.compile(loss=\"mse\", optimizer=\"nadam\")\n",
        "    autoencoder.fit(X_train, X_train, epochs=n_epochs,\n",
        "                    validation_data=(X_valid, X_valid))\n",
        "    return encoder, decoder, encoder(X_train), encoder(X_valid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3TSUkv4yLhE"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "X_train_flat = tf.keras.layers.Flatten()(X_train)\n",
        "X_valid_flat = tf.keras.layers.Flatten()(X_valid)\n",
        "enc1, dec1, X_train_enc1, X_valid_enc1 = train_autoencoder(\n",
        "    100, X_train_flat, X_valid_flat)\n",
        "enc2, dec2, _, _ = train_autoencoder(\n",
        "    30, X_train_enc1, X_valid_enc1, output_activation=\"relu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FpjUrD77yLhE"
      },
      "outputs": [],
      "source": [
        "stacked_ae_1_by_1 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(),\n",
        "    enc1, enc2, dec2, dec1,\n",
        "    tf.keras.layers.Reshape([28, 28])\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KbEe4tQqyLhE"
      },
      "outputs": [],
      "source": [
        "plot_reconstructions(stacked_ae_1_by_1)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlCBejGl2CbS"
      },
      "source": [
        "필요한 경우 전체 적층 오토인코더를 몇 에포크 동안 계속 학습할 수 있습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vRgEnA-WyLhF"
      },
      "outputs": [],
      "source": [
        "stacked_ae_1_by_1.compile(loss=\"mse\", optimizer=\"nadam\")\n",
        "history = stacked_ae_1_by_1.fit(X_train, X_train, epochs=5,\n",
        "                                validation_data=(X_valid, X_valid))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ln1j0u4eyLhF"
      },
      "outputs": [],
      "source": [
        "plot_reconstructions(stacked_ae_1_by_1)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGixWVtuyLhF"
      },
      "source": [
        "## 합성곱 오토인코더"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVK4vETzyLhF"
      },
      "source": [
        "3개의 은닉 층과 1개의 출력 층으로 구성된 적층 오토인코더를 만들어 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qom8ZHdgyLhF"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)  # 추가 코드 - CPU에서 재현성 보장\n",
        "\n",
        "conv_encoder = tf.keras.Sequential([\n",
        "    tf.keras.layers.Reshape([28, 28, 1]),\n",
        "    tf.keras.layers.Conv2D(16, 3, padding=\"same\", activation=\"relu\"),\n",
        "    tf.keras.layers.MaxPool2D(pool_size=2),  # 출력: 14 × 14 x 16\n",
        "    tf.keras.layers.Conv2D(32, 3, padding=\"same\", activation=\"relu\"),\n",
        "    tf.keras.layers.MaxPool2D(pool_size=2),  # 출력: 7 × 7 x 32\n",
        "    tf.keras.layers.Conv2D(64, 3, padding=\"same\", activation=\"relu\"),\n",
        "    tf.keras.layers.MaxPool2D(pool_size=2),  # 출력: 3 × 3 x 64\n",
        "    tf.keras.layers.Conv2D(30, 3, padding=\"same\", activation=\"relu\"),\n",
        "    tf.keras.layers.GlobalAvgPool2D()  # 출력: 30\n",
        "])\n",
        "conv_decoder = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(3 * 3 * 16),\n",
        "    tf.keras.layers.Reshape((3, 3, 16)),\n",
        "    tf.keras.layers.Conv2DTranspose(32, 3, strides=2, activation=\"relu\"),\n",
        "    tf.keras.layers.Conv2DTranspose(16, 3, strides=2, padding=\"same\",\n",
        "                                    activation=\"relu\"),\n",
        "    tf.keras.layers.Conv2DTranspose(1, 3, strides=2, padding=\"same\"),\n",
        "    tf.keras.layers.Reshape([28, 28])\n",
        "])\n",
        "conv_ae = tf.keras.Sequential([conv_encoder, conv_decoder])\n",
        "\n",
        "# 추가 코드 - 모델을 컴파일하고 훈련합니다.\n",
        "conv_ae.compile(loss=\"mse\", optimizer=\"nadam\")\n",
        "history = conv_ae.fit(X_train, X_train, epochs=10,\n",
        "                      validation_data=(X_valid, X_valid))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6p0Yud0RyLhG"
      },
      "outputs": [],
      "source": [
        "# 추가 코드 - 재구성을 표시합니다.\n",
        "plot_reconstructions(conv_ae)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKuHGp1DyLhG"
      },
      "source": [
        "# 추가 자료 - 순환 오토인코더"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3pp3Hxz2CbU"
      },
      "source": [
        "각 패션 MNIST 이미지를 각각 28개의 차원을 가진 28개의 벡터 시퀀스로 취급해 보겠습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZGOuVy0kyLhG"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "recurrent_encoder = tf.keras.Sequential([\n",
        "    tf.keras.layers.LSTM(100, return_sequences=True),\n",
        "    tf.keras.layers.LSTM(30)\n",
        "])\n",
        "recurrent_decoder = tf.keras.Sequential([\n",
        "    tf.keras.layers.RepeatVector(28),\n",
        "    tf.keras.layers.LSTM(100, return_sequences=True),\n",
        "    tf.keras.layers.Dense(28)\n",
        "])\n",
        "recurrent_ae = tf.keras.Sequential([recurrent_encoder, recurrent_decoder])\n",
        "recurrent_ae.compile(loss=\"mse\", optimizer=\"nadam\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4grialnWyLhG",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "history = recurrent_ae.fit(X_train, X_train, epochs=10,\n",
        "                           validation_data=(X_valid, X_valid))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZLOnTctFyLhG"
      },
      "outputs": [],
      "source": [
        "plot_reconstructions(recurrent_ae)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JafadovRyLhH"
      },
      "source": [
        "# 잡음 제거 오토인코더"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdMv5zGbyLhH"
      },
      "source": [
        "드롭아웃 사용:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wTKWjK3EyLhH"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)  # 추가 코드 - CPU에서 재현성 보장\n",
        "\n",
        "dropout_encoder = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(30, activation=\"relu\")\n",
        "])\n",
        "dropout_decoder = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(28 * 28),\n",
        "    tf.keras.layers.Reshape([28, 28])\n",
        "])\n",
        "dropout_ae = tf.keras.Sequential([dropout_encoder, dropout_decoder])\n",
        "\n",
        "# 추가 코드 - 모델을 컴파일하고 훈련합니다.\n",
        "dropout_ae.compile(loss=\"mse\", optimizer=\"nadam\")\n",
        "history = dropout_ae.fit(X_train, X_train, epochs=10,\n",
        "                         validation_data=(X_valid, X_valid))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pucj7MI3yLhI"
      },
      "outputs": [],
      "source": [
        "# 추가 코드\n",
        "tf.random.set_seed(42)\n",
        "dropout = tf.keras.layers.Dropout(0.5)\n",
        "plot_reconstructions(dropout_ae, dropout(X_valid, training=True))\n",
        "save_fig(\"dropout_denoising_plot\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHuVJwq32CbV"
      },
      "source": [
        "원하는 경우 `Dropout` 레이어를 `tf.keras.layers.GaussianNoise(0.2)`로 교체해 볼 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fd8xb5g-yLhI"
      },
      "source": [
        "# 희소 오토인코더"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDseIwd4yLhJ"
      },
      "source": [
        "코딩 레이어에서 시그모이드 활성화 함수를 사용해 보겠습니다. 여기에 $\\ell_1$ 정규화도 추가해 보겠습니다: 이를 위해 코딩 층 뒤에 `ActivityRegularization` 층을 추가합니다. 또는 코딩 층 자체에 `activity_regularizer=tf.keras.regularizers.l1(1e-4)`를 추가할 수도 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ORrSHhNxyLhJ"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)  # 추가 코드 - CPU에서 재현성 보장\n",
        "\n",
        "sparse_l1_encoder = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(300, activation=\"sigmoid\"),\n",
        "    tf.keras.layers.ActivityRegularization(l1=1e-4)\n",
        "])\n",
        "sparse_l1_decoder = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(28 * 28),\n",
        "    tf.keras.layers.Reshape([28, 28])\n",
        "])\n",
        "sparse_l1_ae = tf.keras.Sequential([sparse_l1_encoder, sparse_l1_decoder])\n",
        "\n",
        "# 추가 코드 - 모델을 컴파일하고 훈련합니다.\n",
        "sparse_l1_ae.compile(loss=\"mse\", optimizer=\"nadam\")\n",
        "history = sparse_l1_ae.fit(X_train, X_train, epochs=10,\n",
        "                           validation_data=(X_valid, X_valid))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2vU9twBAyLhK"
      },
      "outputs": [],
      "source": [
        "# 추가 코드 - 재구성을 표시합니다.\n",
        "plot_reconstructions(sparse_l1_ae)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3vxPDGUyLhK"
      },
      "source": [
        "KL 발산 손실과 MAE 및 MSE를 비교하여 그래프를 그려 보겠습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0fOy7aMyLhK"
      },
      "outputs": [],
      "source": [
        "# 추가 코드 - 이 셀은 그림 17-10을 생성하고 저장합니다.\n",
        "p = 0.1\n",
        "q = np.linspace(0.001, 0.999, 500)\n",
        "kl_div = p * np.log(p / q) + (1 - p) * np.log((1 - p) / (1 - q))\n",
        "mse = (p - q) ** 2\n",
        "mae = np.abs(p - q)\n",
        "plt.plot([p, p], [0, 0.3], \"k:\")\n",
        "plt.text(0.05, 0.32, \"타깃\\n희소 정도\", fontsize=14)\n",
        "plt.plot(q, kl_div, \"b-\", label=\"KL 발산\")\n",
        "plt.plot(q, mae, \"g--\", label=r\"MAE ($\\ell_1$)\")\n",
        "plt.plot(q, mse, \"r--\", linewidth=1, label=r\"MSE ($\\ell_2$)\")\n",
        "plt.legend(loc=\"upper left\", fontsize=14)\n",
        "plt.xlabel(\"실제 희소 정도\")\n",
        "plt.ylabel(\"비용\", rotation=0)\n",
        "plt.axis([0, 1, 0, 0.95])\n",
        "plt.grid(True)\n",
        "save_fig(\"sparsity_loss_plot\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYaBIf-22CbW"
      },
      "source": [
        "KL-발산 규제를 위한 사용자 정의 클래스를 정의해 보겠습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-MoRrQGyLhK"
      },
      "outputs": [],
      "source": [
        "kl_divergence = tf.keras.losses.kullback_leibler_divergence\n",
        "\n",
        "class KLDivergenceRegularizer(tf.keras.regularizers.Regularizer):\n",
        "    def __init__(self, weight, target):\n",
        "        self.weight = weight\n",
        "        self.target = target\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        mean_activities = tf.reduce_mean(inputs, axis=0)\n",
        "        return self.weight * (\n",
        "            kl_divergence(self.target, mean_activities) +\n",
        "            kl_divergence(1. - self.target, 1. - mean_activities))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mrp4mzYz2CbX"
      },
      "source": [
        "이제 이 규제를 사용하여 코딩 층에서 모델의 희소 정도가 약 10%가 되도록 해 보겠습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HIoQ-1D4yLhK"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)  # 추가 코드 - CPU에서 재현성 보장\n",
        "\n",
        "kld_reg = KLDivergenceRegularizer(weight=5e-3, target=0.1)\n",
        "sparse_kl_encoder = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(300, activation=\"sigmoid\",\n",
        "                          activity_regularizer=kld_reg)\n",
        "])\n",
        "sparse_kl_decoder = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(28 * 28),\n",
        "    tf.keras.layers.Reshape([28, 28])\n",
        "])\n",
        "sparse_kl_ae = tf.keras.Sequential([sparse_kl_encoder, sparse_kl_decoder])\n",
        "\n",
        "# 추가 코드 - 모델을 컴파일하고 훈련합니다.\n",
        "sparse_kl_ae.compile(loss=\"mse\", optimizer=\"nadam\")\n",
        "history = sparse_kl_ae.fit(X_train, X_train, epochs=10,\n",
        "                           validation_data=(X_valid, X_valid))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0jIPg2ayLhL"
      },
      "outputs": [],
      "source": [
        "# 추가 코드 - 재구성을 표시합니다.\n",
        "plot_reconstructions(sparse_kl_ae)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1WIZRNByLhL"
      },
      "source": [
        "# 변이형 오토인코더"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64ji8G6eyLhL"
      },
      "outputs": [],
      "source": [
        "class Sampling(tf.keras.layers.Layer):\n",
        "    def call(self, inputs):\n",
        "        mean, log_var = inputs\n",
        "        return tf.random.normal(tf.shape(log_var)) * tf.exp(log_var / 2) + mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRO1lU_k2CbY"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)  # 추가 코드 - CPU에서 재현성 보장\n",
        "\n",
        "codings_size = 10\n",
        "\n",
        "inputs = tf.keras.layers.Input(shape=[28, 28])\n",
        "Z = tf.keras.layers.Flatten()(inputs)\n",
        "Z = tf.keras.layers.Dense(150, activation=\"relu\")(Z)\n",
        "Z = tf.keras.layers.Dense(100, activation=\"relu\")(Z)\n",
        "codings_mean = tf.keras.layers.Dense(codings_size)(Z)  # μ\n",
        "codings_log_var = tf.keras.layers.Dense(codings_size)(Z)  # γ\n",
        "codings = Sampling()([codings_mean, codings_log_var])\n",
        "variational_encoder = tf.keras.Model(\n",
        "    inputs=[inputs], outputs=[codings_mean, codings_log_var, codings])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gc-J-lRz2CbY"
      },
      "outputs": [],
      "source": [
        "decoder_inputs = tf.keras.layers.Input(shape=[codings_size])\n",
        "x = tf.keras.layers.Dense(100, activation=\"relu\")(decoder_inputs)\n",
        "x = tf.keras.layers.Dense(150, activation=\"relu\")(x)\n",
        "x = tf.keras.layers.Dense(28 * 28)(x)\n",
        "outputs = tf.keras.layers.Reshape([28, 28])(x)\n",
        "variational_decoder = tf.keras.Model(inputs=[decoder_inputs], outputs=[outputs])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05ub8G912CbY"
      },
      "outputs": [],
      "source": [
        "_, _, codings = variational_encoder(inputs)\n",
        "reconstructions = variational_decoder(codings)\n",
        "variational_ae = tf.keras.Model(inputs=[inputs], outputs=[reconstructions])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_mrDaiwj2CbY"
      },
      "outputs": [],
      "source": [
        "latent_loss = -0.5 * tf.reduce_sum(\n",
        "    1 + codings_log_var - tf.exp(codings_log_var) - tf.square(codings_mean),\n",
        "    axis=-1)\n",
        "variational_ae.add_loss(tf.reduce_mean(latent_loss) / 784.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YXUKW_UPyLhL"
      },
      "outputs": [],
      "source": [
        "variational_ae.compile(loss=\"mse\", optimizer=\"nadam\")\n",
        "history = variational_ae.fit(X_train, X_train, epochs=25, batch_size=128,\n",
        "                             validation_data=(X_valid, X_valid))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FDs525puyLhL",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "plot_reconstructions(variational_ae)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJfl2bXbyLhM"
      },
      "source": [
        "## 패션 이미지 생성"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCHEtBgf2CbZ"
      },
      "source": [
        "몇 가지 무작위 코딩을 생성하고 디코딩해 보겠습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mBnytW9O2CbZ"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)  # 추가 코드 - CPU에서 재현성 보장\n",
        "\n",
        "codings = tf.random.normal(shape=[3 * 7, codings_size])\n",
        "images = variational_decoder(codings).numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0j6dpeq2CbZ"
      },
      "source": [
        "이제 이 이미지를 출력해 보겠습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gzwLpDIDyLhM"
      },
      "outputs": [],
      "source": [
        "# 추가 코드 - 이 셀은 그림 17-12를 생성하고 저장합니다.\n",
        "\n",
        "def plot_multiple_images(images, n_cols=None):\n",
        "    n_cols = n_cols or len(images)\n",
        "    n_rows = (len(images) - 1) // n_cols + 1\n",
        "    if images.shape[-1] == 1:\n",
        "        images = images.squeeze(axis=-1)\n",
        "    plt.figure(figsize=(n_cols, n_rows))\n",
        "    for index, image in enumerate(images):\n",
        "        plt.subplot(n_rows, n_cols, index + 1)\n",
        "        plt.imshow(image, cmap=\"binary\")\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "plot_multiple_images(images, 7)\n",
        "save_fig(\"vae_generated_images_plot\", tight_layout=False)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JLZ_HWqyLhM"
      },
      "source": [
        "이제 두 이미지 사이에 시맨틱 보간을 수행해 보겠습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GsVXES9byLhM"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)  # 추가 코드 - CPU에서 재현성 보장\n",
        "\n",
        "codings = np.zeros([7, codings_size])\n",
        "codings[:, 3] = np.linspace(-0.8, 0.8, 7)  # 이 경우 축 3이 가장 좋습니다.\n",
        "images = variational_decoder(codings).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8s32sLd2Cba"
      },
      "outputs": [],
      "source": [
        "# 추가 코드 - 이 셀은 그림 17-13을 생성하고 저장합니다.\n",
        "plot_multiple_images(images)\n",
        "save_fig(\"semantic_interpolation_plot\", tight_layout=False)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hA5xhT_yLhN"
      },
      "source": [
        "# 생성적 적대 신경망"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N31KxgksyLhN"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)  # 추가 코드 - CPU에서 재현성 보장\n",
        "\n",
        "codings_size = 30\n",
        "\n",
        "Dense = tf.keras.layers.Dense\n",
        "generator = tf.keras.Sequential([\n",
        "    Dense(100, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
        "    Dense(150, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
        "    Dense(28 * 28, activation=\"sigmoid\"),\n",
        "    tf.keras.layers.Reshape([28, 28])\n",
        "])\n",
        "discriminator = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(),\n",
        "    Dense(150, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
        "    Dense(100, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
        "    Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "gan = tf.keras.Sequential([generator, discriminator])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qm4Uk3MTyLhN"
      },
      "outputs": [],
      "source": [
        "discriminator.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\")\n",
        "discriminator.trainable = False\n",
        "gan.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQoPjJthyLhN"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "dataset = tf.data.Dataset.from_tensor_slices(X_train).shuffle(1000)\n",
        "dataset = dataset.batch(batch_size, drop_remainder=True).prefetch(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxrUUhrjyLhN"
      },
      "outputs": [],
      "source": [
        "def train_gan(gan, dataset, batch_size, codings_size, n_epochs):\n",
        "    generator, discriminator = gan.layers\n",
        "    for epoch in range(n_epochs):\n",
        "        print(f\"에포크 {epoch + 1}/{n_epochs}\")  # 추가 코드\n",
        "        for X_batch in dataset:\n",
        "            # 1단계 - 판별자 훈련\n",
        "            noise = tf.random.normal(shape=[batch_size, codings_size])\n",
        "            generated_images = generator(noise)\n",
        "            X_fake_and_real = tf.concat([generated_images, X_batch], axis=0)\n",
        "            y1 = tf.constant([[0.]] * batch_size + [[1.]] * batch_size)\n",
        "            discriminator.train_on_batch(X_fake_and_real, y1)\n",
        "            # 2단계 - 생성자 훈련\n",
        "            noise = tf.random.normal(shape=[batch_size, codings_size])\n",
        "            y2 = tf.constant([[1.]] * batch_size)\n",
        "            gan.train_on_batch(noise, y2)\n",
        "        # 추가 코드 - 훈련 중 이미지 출력\n",
        "        plot_multiple_images(generated_images.numpy(), 8)\n",
        "        plt.show()\n",
        "\n",
        "# 코랩에서 메모리 부족 에러를 피하기 위해 n_epochs를 50에서 10으로 바꿉니다.\n",
        "train_gan(gan, dataset, batch_size, codings_size, n_epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FoUbjM5b2Cbb"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)  # 추가 코드 - CPU에서 재현성 보장\n",
        "\n",
        "codings = tf.random.normal(shape=[batch_size, codings_size])\n",
        "generated_images = generator.predict(codings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NrLzMxweyLhO"
      },
      "outputs": [],
      "source": [
        "# 추가 코드 - 이 셀은 그림 17-15를 생성하고 저장합니다.\n",
        "plot_multiple_images(generated_images, 8)\n",
        "save_fig(\"gan_generated_images_plot\", tight_layout=False)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZN7cSsKyLhO"
      },
      "source": [
        "# 심층 합성곱 GAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NgT77mdVyLhO"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)  # 추가 코드 - CPU에서 재현성 보장\n",
        "\n",
        "codings_size = 100\n",
        "\n",
        "generator = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(7 * 7 * 128),\n",
        "    tf.keras.layers.Reshape([7, 7, 128]),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Conv2DTranspose(64, kernel_size=5, strides=2,\n",
        "                                    padding=\"same\", activation=\"relu\"),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Conv2DTranspose(1, kernel_size=5, strides=2,\n",
        "                                    padding=\"same\", activation=\"tanh\"),\n",
        "])\n",
        "discriminator = tf.keras.Sequential([\n",
        "    tf.keras.layers.Conv2D(64, kernel_size=5, strides=2, padding=\"same\",\n",
        "                        activation=tf.keras.layers.LeakyReLU(0.2)),\n",
        "    tf.keras.layers.Dropout(0.4),\n",
        "    tf.keras.layers.Conv2D(128, kernel_size=5, strides=2, padding=\"same\",\n",
        "                        activation=tf.keras.layers.LeakyReLU(0.2)),\n",
        "    tf.keras.layers.Dropout(0.4),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "gan = tf.keras.Sequential([generator, discriminator])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-Sj_zc9yLhO"
      },
      "outputs": [],
      "source": [
        "# 추가 코드 - 앞서와 같이 판별자와 gan을 컴파일합니다.\n",
        "discriminator.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\")\n",
        "discriminator.trainable = False\n",
        "gan.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Ss7zwLGyLhO"
      },
      "outputs": [],
      "source": [
        "X_train_dcgan = X_train.reshape(-1, 28, 28, 1) * 2. - 1. # 크기 변경 및 스케일 조정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xKWHUiUqyLhP"
      },
      "outputs": [],
      "source": [
        "# 추가 코드 - 앞서와 마찬가지로 데이터셋을 생성하고 GAN을 학습시킵니다.\n",
        "batch_size = 32\n",
        "dataset = tf.data.Dataset.from_tensor_slices(X_train_dcgan)\n",
        "dataset = dataset.shuffle(1000)\n",
        "dataset = dataset.batch(batch_size, drop_remainder=True).prefetch(1)\n",
        "# 코랩에서 메모리 부족 에러를 피하기 위해 n_epochs를 50에서 10으로 바꿉니다.\n",
        "train_gan(gan, dataset, batch_size, codings_size, n_epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yY8_xytVyLhP"
      },
      "outputs": [],
      "source": [
        "# 추가 코드 - 이 셀은 그림 17-16을 생성하고 저장합니다.\n",
        "tf.random.set_seed(42)\n",
        "noise = tf.random.normal(shape=[batch_size, codings_size])\n",
        "generated_images = generator.predict(noise)\n",
        "plot_multiple_images(generated_images, 8)\n",
        "save_fig(\"dcgan_generated_images_plot\", tight_layout=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUCR2FrdyLhP"
      },
      "source": [
        "# 확산 모델"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyvKvczoil-7"
      },
      "source": [
        "데이터셋의 이미지로 시작하여 각 타임 스텝 $t$에서 확산 과정은 평균 0, 분산 $\\beta_t$의 가우스 잡음을 추가합니다. 그런 다음 모델은 이 프로세스를 역으로 수행하도록 훈련됩니다. 보다 구체적으로, 정방향 과정에 의해 생성된 잡음 이미지와 시간 $t$가 주어지면 모델은 분산 1로 스케일링된 원본 이미지에 추가된 총 잡음을 예측하도록 훈련됩니다.\n",
        "\n",
        "[DDPM 논문](https://arxiv.org/abs/2006.11239)에서는 $\\beta_1$ = 0.0001에서 $\\beta_T = $0.02($T$가 최대 스텝)로 $\\beta_t$를 늘렸지만, [개선된 DDPM 논문](https://arxiv.org/pdf/2102.09672.pdf)에서는 $\\bar{\\alpha_t} = \\prod_{i=0}^{t}$를 점차적으로 감소시키는 다음과 같은 $\\cos^2(\\ldots)$ 스케줄 사용을 제안했습니다. $\\alpha_i$ 를 1에서 0으로, 여기서 $\\alpha_t = 1 - \\beta_t$ 로 점차 감소시킵니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yxPNTP3QpHAw"
      },
      "outputs": [],
      "source": [
        "def variance_schedule(T, s=0.008, max_beta=0.999):\n",
        "    t = np.arange(T + 1)\n",
        "    f = np.cos((t / T + s) / (1 + s) * np.pi / 2) ** 2\n",
        "    alpha = np.clip(f[1:] / f[:-1], 1 - max_beta, 1)\n",
        "    alpha = np.append(1, alpha).astype(np.float32)  # add α₀ = 1\n",
        "    beta = 1 - alpha\n",
        "    alpha_cumprod = np.cumprod(alpha)\n",
        "    return alpha, alpha_cumprod, beta  # αₜ , α̅ₜ , βₜ for t = 0 to T\n",
        "\n",
        "np.random.seed(42)  # 추가 코드 - 재현성을 위한\n",
        "T = 4000\n",
        "alpha, alpha_cumprod, beta = variance_schedule(T)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4ijMDaVkyXi"
      },
      "source": [
        "DDPM 논문에서 저자들은 $T = 1,000$을 사용했지만, 개선된 DDPM에서는 이 값을 $T = 4,000$으로 늘렸기 때문에 이 값을 사용합니다. `alpha` 변수는 $\\alpha_0, \\alpha_1, ..., \\alpha_T$를 포함하는 벡터입니다. 변수 `alpha_cumprod`는 $\\bar{\\alpha_0}, \\bar{\\alpha_1}, ..., \\bar{\\alpha_T}$를 포함하는 벡터입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9z6rfMntnSMi"
      },
      "source": [
        "`alpha_cumprod`을 그래프로 그려보죠:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zd442V1g2Cbf"
      },
      "outputs": [],
      "source": [
        "# 추가 코드\n",
        "plt.figure(figsize=(6, 3))\n",
        "plt.plot(beta, \"r--\", label=r\"$\\beta_t$\")\n",
        "plt.plot(alpha_cumprod, \"b\", label=r\"$\\bar{\\alpha}_t$\")\n",
        "plt.axis([0, T, 0, 1])\n",
        "plt.grid(True)\n",
        "plt.xlabel(r\"t\")\n",
        "plt.legend()\n",
        "save_fig(\"variance_schedule_plot\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZL7ZBkTKndy-"
      },
      "source": [
        "`prepare_batch()` 함수는 이미지 배치를 가져와 각 이미지에 대해 1에서 $T$ 사이의 다른 임의 시간을 사용하여 각 이미지에 잡음을 추가하고 입력과 타겟을 포함하는 튜플을 반환합니다:\n",
        "\n",
        "* 입력은 잡음이 있는 이미지와 해당 시간이 포함된 `dict`입니다. 이 함수는 DDPM 논문의 방정식 (4)를 사용하여 원본 이미지에서 직접 노이즈가 있는 이미지를 한 번에 계산합니다. 이는 정방향 확산 과정의 지름길입니다.\n",
        "* 타깃은 잡음 이미지를 생성하는 데 사용된 잡음입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i30a1HYw2Cbf"
      },
      "outputs": [],
      "source": [
        "def prepare_batch(X):\n",
        "    X = tf.cast(X[..., tf.newaxis], tf.float32) * 2 - 1  # -1에서 +1까지 스케일링\n",
        "    X_shape = tf.shape(X)\n",
        "    t = tf.random.uniform([X_shape[0]], minval=1, maxval=T + 1, dtype=tf.int32)\n",
        "    alpha_cm = tf.gather(alpha_cumprod, t)\n",
        "    alpha_cm = tf.reshape(alpha_cm, [X_shape[0]] + [1] * (len(X_shape) - 1))\n",
        "    noise = tf.random.normal(X_shape)\n",
        "    return {\n",
        "        \"X_noisy\": alpha_cm ** 0.5 * X + (1 - alpha_cm) ** 0.5 * noise,\n",
        "        \"time\": t,\n",
        "    }, noise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKFKjTAI2Cbf"
      },
      "source": [
        "이제 훈련과 검증을 위한 `tf.data.Dataset`을 준비합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i9qt5HPZ8rhv"
      },
      "outputs": [],
      "source": [
        "def prepare_dataset(X, batch_size=32, shuffle=False):\n",
        "    ds = tf.data.Dataset.from_tensor_slices(X)\n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(10_000)\n",
        "    return ds.batch(batch_size).map(prepare_batch).prefetch(1)\n",
        "\n",
        "tf.random.set_seed(43)  # 추가 코드 - CPU에서 재현성 보장\n",
        "train_set = prepare_dataset(X_train, batch_size=32, shuffle=True)\n",
        "valid_set = prepare_dataset(X_valid, batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAxL8EmF3Hcl"
      },
      "source": [
        "간단한 모델 상태를 확인하기 위해 몇 가지 훈련 샘플과 예측할 잡음, 원본 이미지(적절하게 스케일링된 잡음 이미지에서 적절하게 스케일링된 잡음을 빼서 얻은 이미지)를 살펴 보겠습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8mwYDDtz6Zm"
      },
      "outputs": [],
      "source": [
        "# 추가 코드 - 간단한 데이터 검증\n",
        "\n",
        "def subtract_noise(X_noisy, time, noise):\n",
        "    X_shape = tf.shape(X_noisy)\n",
        "    alpha_cm = tf.gather(alpha_cumprod, time)\n",
        "    alpha_cm = tf.reshape(alpha_cm, [X_shape[0]] + [1] * (len(X_shape) - 1))\n",
        "    return (X_noisy - (1 - alpha_cm) ** 0.5 * noise) / alpha_cm ** 0.5\n",
        "\n",
        "X_dict, Y_noise = list(train_set.take(1))[0]  # get the first batch\n",
        "X_original = subtract_noise(X_dict[\"X_noisy\"], X_dict[\"time\"], Y_noise)\n",
        "\n",
        "print(\"원본 이미지\")\n",
        "plot_multiple_images(X_original[:8].numpy())\n",
        "plt.show()\n",
        "print(\"타임 스텝:\", X_dict[\"time\"].numpy()[:8])\n",
        "print(\"잡음 이미지\")\n",
        "plot_multiple_images(X_dict[\"X_noisy\"][:8].numpy())\n",
        "plt.show()\n",
        "print(\"예측할 잡음\")\n",
        "plot_multiple_images(Y_noise[:8].numpy())\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcBpIkNxrPw9"
      },
      "source": [
        "이제 확산 모델 자체를 구축할 준비가 되었습니다. 이미지와 시간을 모두 처리해야 합니다. DDPM 논문에서 제안한 대로 [트랜스포머](https://arxiv.org/abs/1706.03762) 논문에서처럼 사인파 인코딩을 사용하여 시간을 인코딩합니다. 시간 인덱스(정수)를 나타내는 _m_ 정수의 벡터가 주어지면 이 층은 _m_ × _d_ 행렬을 반환하며, 여기서 _d_는 선택한 임베딩 크기입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0HGLhS4oNkNC"
      },
      "outputs": [],
      "source": [
        "# 추가 코드 - 사용자 정의 시간 인코딩 층을 구현합니다.\n",
        "\n",
        "embed_size = 64\n",
        "\n",
        "class TimeEncoding(tf.keras.layers.Layer):\n",
        "    def __init__(self, T, embed_size, dtype=tf.float32, **kwargs):\n",
        "        super().__init__(dtype=dtype, **kwargs)\n",
        "        assert embed_size % 2 == 0, \"embed_size must be even\"\n",
        "        p, i = np.meshgrid(np.arange(T + 1), 2 * np.arange(embed_size // 2))\n",
        "        t_emb = np.empty((T + 1, embed_size))\n",
        "        t_emb[:, ::2] = np.sin(p / 10_000 ** (i / embed_size)).T\n",
        "        t_emb[:, 1::2] = np.cos(p / 10_000 ** (i / embed_size)).T\n",
        "        self.time_encodings = tf.constant(t_emb.astype(self.dtype))\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return tf.gather(self.time_encodings, inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18SYrUNysJ62"
      },
      "source": [
        "이제 모델을 빌드해 보겠습니다. 개선된 DDPM 문서에서는 UNet 모델을 사용합니다. `Conv2D` + `BatchNormalization` 층을 통해 이미지를 처리하고 스킵 연결을 가진 UNet과 유사한 모델을 만들어 보겠습니다. 점차적으로 이미지 다운샘플링(`strides=2`인 `MaxPooling` 층을 사용)한 다음 다시 업샘플링합니다(`Upsampling2D` 층 사용). 다운샘플링 부분과 업샘플링 부분에 스킵 연결이 추가됩니다. 또한 `Dense` 레이어를 통과하여 올바른 크기로 크기를 조정한 후 각 블록의 출력에 시간 인코딩을 추가합니다.\n",
        "\n",
        "* **참고**: 이미지의 시간 인코딩은 마지막 축(채널)을 따라 이미지의 모든 픽셀에 추가됩니다. 따라서 `Conv2D` 층의 유닛 수는 임베딩 크기와 일치해야 하며, `time_enc` 텐서를 재구성하여 너비 및 높이 차원을 추가해야 합니다.\n",
        "* 이 UNet 구현은 keras.io의 [이미지 분할 예제](https://keras.io/examples/vision/oxford_pets_image_segmentation/)와 [공식 확산 모델 구현](https://github.com/hojonathanho/diffusion/blob/master/diffusion_tf/models/unet.py)을 참고했습니다. 첫 번째 구현과 비교하여 몇 가지를 추가했는데, 특히 시간 인코딩과 다운/업 부분의 스킵 연결이 추가되었습니다. 두 번째 구현과 비교하여 몇 가지 사항, 특히 주의 레이어를 제거했습니다. 패션 MNIST에는 과한 것 같지만 자유롭게 추가할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4QLnc8wkfXrh"
      },
      "outputs": [],
      "source": [
        "def build_diffusion_model():\n",
        "    X_noisy = tf.keras.layers.Input(shape=[28, 28, 1], name=\"X_noisy\")\n",
        "    time_input = tf.keras.layers.Input(shape=[], dtype=tf.int32, name=\"time\")\n",
        "    time_enc = TimeEncoding(T, embed_size)(time_input)\n",
        "\n",
        "    dim = 16\n",
        "    Z = tf.keras.layers.ZeroPadding2D((3, 3))(X_noisy)\n",
        "    Z = tf.keras.layers.Conv2D(dim, 3)(Z)\n",
        "    Z = tf.keras.layers.BatchNormalization()(Z)\n",
        "    Z = tf.keras.layers.Activation(\"relu\")(Z)\n",
        "\n",
        "    time = tf.keras.layers.Dense(dim)(time_enc)  # 시간 인코딩 적용\n",
        "    Z = time[:, tf.newaxis, tf.newaxis, :] + Z  # 모든 픽셀에 시간 데이터 추가\n",
        "\n",
        "    skip = Z\n",
        "    cross_skips = []  # UNet의 다운샘플링 & 업샘플링을 가로지르는 스킵 연결\n",
        "\n",
        "    for dim in (32, 64, 128):\n",
        "        Z = tf.keras.layers.Activation(\"relu\")(Z)\n",
        "        Z = tf.keras.layers.SeparableConv2D(dim, 3, padding=\"same\")(Z)\n",
        "        Z = tf.keras.layers.BatchNormalization()(Z)\n",
        "\n",
        "        Z = tf.keras.layers.Activation(\"relu\")(Z)\n",
        "        Z = tf.keras.layers.SeparableConv2D(dim, 3, padding=\"same\")(Z)\n",
        "        Z = tf.keras.layers.BatchNormalization()(Z)\n",
        "\n",
        "        cross_skips.append(Z)\n",
        "        Z = tf.keras.layers.MaxPooling2D(3, strides=2, padding=\"same\")(Z)\n",
        "        skip_link = tf.keras.layers.Conv2D(dim, 1, strides=2,\n",
        "                                           padding=\"same\")(skip)\n",
        "        Z = tf.keras.layers.add([Z, skip_link])\n",
        "\n",
        "        time = tf.keras.layers.Dense(dim)(time_enc)\n",
        "        Z = time[:, tf.newaxis, tf.newaxis, :] + Z\n",
        "        skip = Z\n",
        "\n",
        "    for dim in (64, 32, 16):\n",
        "        Z = tf.keras.layers.Activation(\"relu\")(Z)\n",
        "        Z = tf.keras.layers.Conv2DTranspose(dim, 3, padding=\"same\")(Z)\n",
        "        Z = tf.keras.layers.BatchNormalization()(Z)\n",
        "\n",
        "        Z = tf.keras.layers.Activation(\"relu\")(Z)\n",
        "        Z = tf.keras.layers.Conv2DTranspose(dim, 3, padding=\"same\")(Z)\n",
        "        Z = tf.keras.layers.BatchNormalization()(Z)\n",
        "\n",
        "        Z = tf.keras.layers.UpSampling2D(2)(Z)\n",
        "\n",
        "        skip_link = tf.keras.layers.UpSampling2D(2)(skip)\n",
        "        skip_link = tf.keras.layers.Conv2D(dim, 1, padding=\"same\")(skip_link)\n",
        "        Z = tf.keras.layers.add([Z, skip_link])\n",
        "\n",
        "        time = tf.keras.layers.Dense(dim)(time_enc)\n",
        "        Z = time[:, tf.newaxis, tf.newaxis, :] + Z\n",
        "        Z = tf.keras.layers.concatenate([Z, cross_skips.pop()], axis=-1)\n",
        "        skip = Z\n",
        "\n",
        "    outputs = tf.keras.layers.Conv2D(1, 3, padding=\"same\")(Z)[:, 2:-2, 2:-2]\n",
        "    return tf.keras.Model(inputs=[X_noisy, time_input], outputs=[outputs])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R60NMcUT0b1K"
      },
      "source": [
        "모델을 훈련시켜 봅시다!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5chylX-Vp9te"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)  # 추가 코드 - CPU에서 재현성 보장\n",
        "model = build_diffusion_model()\n",
        "model.compile(loss=tf.keras.losses.Huber(), optimizer=\"nadam\")\n",
        "\n",
        "# 추가 코드 - 모델 체크포인트 콜백을 추가합니다.\n",
        "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"my_diffusion_model\",\n",
        "                                                   save_best_only=True)\n",
        "\n",
        "history = model.fit(train_set, validation_data=valid_set, epochs=100,\n",
        "                    callbacks=[checkpoint_cb])  # 추가 코드"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuPHepTY2Cbi"
      },
      "source": [
        "모델이 훈련되면 이를 사용하여 새 이미지를 생성할 수 있습니다. 이를 위해 가우스 잡음을 생성하고 이것이 시간 $T$ 동안의 확산 과정의 결과라고 가정합니다. 그런 다음 모델을 사용하여 $T - 1$ 시점의 이미지를 예측한 다음 다시 호출하여 $T - 2$를 구하는 식으로 각 단계에서 약간의 노이즈를 제거합니다. 마지막으로 패션 MNIST 데이터셋에서 가져온 것처럼 보이는 이미지를 얻습니다. 이 역방향 과정에 대한 방정식은 DDPM 논문 4페이지 상단에 있습니다(알고리즘 2의 4단계)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMXxz4qV8Luk"
      },
      "outputs": [],
      "source": [
        "def generate(model, batch_size=32):\n",
        "    X = tf.random.normal([batch_size, 28, 28, 1])\n",
        "    for t in range(T - 1, 0, -1):\n",
        "        print(f\"\\rt = {t}\", end=\" \")  # 추가 코드 - 진행률 표시\n",
        "        noise = (tf.random.normal if t > 1 else tf.zeros)(tf.shape(X))\n",
        "        X_noise = model({\"X_noisy\": X, \"time\": tf.constant([t] * batch_size)})\n",
        "        X = (\n",
        "            1 / alpha[t] ** 0.5\n",
        "            * (X - beta[t] / (1 - alpha_cumprod[t]) ** 0.5 * X_noise)\n",
        "            + (1 - alpha[t]) ** 0.5 * noise\n",
        "        )\n",
        "    return X\n",
        "\n",
        "tf.random.set_seed(42)  # 추가 코드 - CPU에서 재현성 보장\n",
        "X_gen = generate(model)  # 생성된 이미지"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lfMa2NVwJuzE"
      },
      "outputs": [],
      "source": [
        "plot_multiple_images(X_gen.numpy(), 8)\n",
        "save_fig(\"ddpm_generated_images_plot\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkV6UZym2Cbi"
      },
      "source": [
        "이 이미지 중 일부는 정말 설득력이 있습니다! 확산 모델은 GAN에 비해 더 다양한 이미지를 생성하는 경향이 있으며, 이미지 품질에서도 GAN을 능가합니다. 또한 훈련이 훨씬 더 안정적입니다. 하지만 이미지를 생성하는 데는 시간이 훨씬 더 오래 걸립니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKyC0R9dyLhV"
      },
      "source": [
        "# 추가 자료 - 이진 오토인코더를 사용한 해싱"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDE4f-XpyLhW"
      },
      "source": [
        "인코더에 16개의 뉴런으로 구성된 출력 층과 시그모이드 활성화 함수를 사용하고 바로 그 전에 가우스 잡음을 추가한 오토인코더를 훈련해 보겠습니다. 훈련 중에 잡음 층은 이전 층이 큰 값을 출력하도록 권장합니다. 작은 값은 잡음에 의해 희석되기 때문입니다. 그리고 시그모이드 활성화 함수 덕분에 출력 층이 0 또는 1에 가까운 값을 출력하게 됩니다. 출력 값을 0과 1로 반올림하면 16비트 \"시맨틱\" 해시를 얻을 수 있습니다. 모든 것이 잘 작동하면 비슷해 보이는 이미지의 해시값이 동일합니다. 이는 검색 엔진에 매우 유용할 수 있습니다: 예를 들어 이미지의 시맨틱 해시로 식별되는 서버에 각 이미지를 저장하면 유사한 이미지가 모두 같은 서버에 저장됩니다. 검색 엔진 사용자가 검색할 이미지를 제공하면 검색 엔진이 인코더를 사용하여 이미지의 해시를 계산하고 해당 해시로 식별되는 서버의 모든 이미지를 신속하게 반환합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3DRvS7twyLhW"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "hashing_encoder = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
        "    tf.keras.layers.GaussianNoise(15.),\n",
        "    tf.keras.layers.Dense(16, activation=\"sigmoid\"),\n",
        "])\n",
        "hashing_decoder = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(28 * 28),\n",
        "    tf.keras.layers.Reshape([28, 28])\n",
        "])\n",
        "hashing_ae = tf.keras.Sequential([hashing_encoder, hashing_decoder])\n",
        "hashing_ae.compile(loss=\"mse\", optimizer=\"nadam\")\n",
        "history = hashing_ae.fit(X_train, X_train, epochs=10,\n",
        "                         validation_data=(X_valid, X_valid))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OpQ1O-qyLhW"
      },
      "source": [
        "이 오토인코더는 정보를 너무 많이 압축하여(16비트까지!) 손실이 크지만 괜찮습니다. 이미지를 완벽하게 재구성하는 것이 아니라 시맨틱 해시를 생성하는 데 사용하므로 괜찮습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HyOsfAdiyLhW"
      },
      "outputs": [],
      "source": [
        "plot_reconstructions(hashing_ae)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8Q_Rx-PyLhX"
      },
      "source": [
        "이제 검증 세트의 처음 몇 개의 이미지에 대한 해시가 어떻게 보이는지 살펴보겠습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Akb6fNptyLhX"
      },
      "outputs": [],
      "source": [
        "hashes = hashing_encoder.predict(X_valid).round().astype(np.int32)\n",
        "hashes *= np.array([[2 ** bit for bit in range(16)]])\n",
        "hashes = hashes.sum(axis=1)\n",
        "for h in hashes[:5]:\n",
        "    print(f\"{h:016b}\")\n",
        "print(\"...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5n8N4RoyLhX"
      },
      "source": [
        "이제 유효성 검사 집합에서 가장 일반적인 이미지 해시를 찾고 각 해시에 대해 몇 개의 이미지를 표시해 보겠습니다. 다음 이미지에서는 주어진 행에 있는 모든 이미지의 해시가 동일합니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hS80DfcLyLhX"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "n_hashes = 10\n",
        "n_images = 8\n",
        "\n",
        "top_hashes = Counter(hashes).most_common(n_hashes)\n",
        "\n",
        "plt.figure(figsize=(n_images, n_hashes))\n",
        "for hash_index, (image_hash, hash_count) in enumerate(top_hashes):\n",
        "    indices = (hashes == image_hash)\n",
        "    for index, image in enumerate(X_valid[indices][:n_images]):\n",
        "        plt.subplot(n_hashes, n_images, hash_index * n_images + index + 1)\n",
        "        plt.imshow(image, cmap=\"binary\")\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J77AxVz5yLhX"
      },
      "source": [
        "# 연습문제 해답"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIf1WVKcyLhX"
      },
      "source": [
        "## 1. to 8."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GXqpaDHyLhY"
      },
      "source": [
        "## 10.\n",
        "_문제: 잡음 제거 오토인코더를 사용해 이미지 분류기를 사전훈련해보세요. (간단하게) MNIST를 사용하거나 도전적인 문제를 원한다면 CIFAR10 같은 좀 더 복잡한 이미지 데이터셋을 사용할 수 있습니다. 어떤 데이터셋을 사용하던지 다음 단계를 따르세요._\n",
        "\n",
        "* 데이터셋을 훈련 세트와 테스트 세트로 나눕니다. 전체 훈련 세트에서 심층 잡음 제거 오토인코더를 훈련합니다.\n",
        "* 이미지가 잘 재구성되는 지 확인하세요. 코딩 층의 각 뉴런을 가장 크게 활성화하는 이미지를 시각화해보세요.\n",
        "* 이 오토인코더의 아래 층을 재사용해 분류 DNN을 만드세요. 훈련 세트에서 이미지 500개만 사용해 훈련합니다. 사전훈련을 사용하는 것이 더 나은가요? 사용하지 않는 것이 더 나은가요?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RVAVmIznyLhY"
      },
      "outputs": [],
      "source": [
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "X_train = X_train / 255\n",
        "X_test = X_test / 255"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXcft-kHyLhY"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "denoising_encoder = tf.keras.Sequential([\n",
        "    tf.keras.layers.GaussianNoise(0.1),\n",
        "    tf.keras.layers.Conv2D(32, 3, padding=\"same\", activation=\"relu\"),\n",
        "    tf.keras.layers.MaxPool2D(),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(512, activation=\"relu\"),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2lC7XBqyLhY"
      },
      "outputs": [],
      "source": [
        "denoising_decoder = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(16 * 16 * 32, activation=\"relu\"),\n",
        "    tf.keras.layers.Reshape([16, 16, 32]),\n",
        "    tf.keras.layers.Conv2DTranspose(filters=3, kernel_size=3, strides=2,\n",
        "                                 padding=\"same\", activation=\"sigmoid\")\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RgtxA9BjyLhY"
      },
      "outputs": [],
      "source": [
        "denoising_ae = tf.keras.Sequential([denoising_encoder, denoising_decoder])\n",
        "denoising_ae.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\",\n",
        "                     metrics=[\"mse\"])\n",
        "history = denoising_ae.fit(X_train, X_train, epochs=10,\n",
        "                           validation_data=(X_test, X_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTlFyrmSyLhY"
      },
      "outputs": [],
      "source": [
        "n_images = 5\n",
        "new_images = X_test[:n_images]\n",
        "new_images_noisy = new_images + np.random.randn(n_images, 32, 32, 3) * 0.1\n",
        "new_images_denoised = denoising_ae.predict(new_images_noisy)\n",
        "\n",
        "plt.figure(figsize=(6, n_images * 2))\n",
        "for index in range(n_images):\n",
        "    plt.subplot(n_images, 3, index * 3 + 1)\n",
        "    plt.imshow(new_images[index])\n",
        "    plt.axis('off')\n",
        "    if index == 0:\n",
        "        plt.title(\"Original\")\n",
        "    plt.subplot(n_images, 3, index * 3 + 2)\n",
        "    plt.imshow(new_images_noisy[index].clip(0., 1.))\n",
        "    plt.axis('off')\n",
        "    if index == 0:\n",
        "        plt.title(\"Noisy\")\n",
        "    plt.subplot(n_images, 3, index * 3 + 3)\n",
        "    plt.imshow(new_images_denoised[index])\n",
        "    plt.axis('off')\n",
        "    if index == 0:\n",
        "        plt.title(\"Denoised\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddB3aKyFyLhZ"
      },
      "source": [
        "## 11.\n",
        "_문제: 이미지 데이터셋을 하나 선택해 변이형 오토인코더를 훈련하고 이미지를 생성해보세요. 또는 관심있는 레이블이 없는 데이터셋을 찾아서 새로운 샘플을 생성할 수 있는지 확인해 보세요._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEfVD1TTyLhZ"
      },
      "source": [
        "VAE 코드 참조"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mePWI8ryLhZ"
      },
      "source": [
        "## 12.\n",
        "_문제: 이미지 데이터셋을 처리하는 DCGAN을 훈련하고 이를 사용해 이미지를 생성해보세요. 경험 재생을 추가하고 도움이 되는지 확인하세요. 생성된 클래스를 제어할 수 있는 조건 GAN으로 바꾸어 시도해보세요._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWBNDAFvyLhZ"
      },
      "source": [
        "TODO"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "nav_menu": {
      "height": "381px",
      "width": "453px"
    },
    "toc": {
      "navigate_menu": true,
      "number_sections": true,
      "sideBar": true,
      "threshold": 6,
      "toc_cell": false,
      "toc_section_display": "block",
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}