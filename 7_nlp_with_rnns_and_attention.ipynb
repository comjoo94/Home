{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvGItwuDC45m"
      },
      "source": [
        "**7장 – RNN과 어텐션을 사용한 자연어 처리**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFXIv9qNpKzt",
        "tags": []
      },
      "source": [
        "# 설정"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IPbJEmZpKzu"
      },
      "source": [
        "이 프로젝트에는 Python 3.7 이상이 필요합니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TFSU3FCOpKzu"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "assert sys.version_info >= (3, 7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJtVEqxfpKzw"
      },
      "source": [
        "그리고 TensorFlow ≥ 2.8:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Piq5se2pKzx"
      },
      "outputs": [],
      "source": [
        "from packaging import version\n",
        "import tensorflow as tf\n",
        "\n",
        "assert version.parse(tf.__version__) >= version.parse(\"2.8.0\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDaDoLQTpKzx"
      },
      "source": [
        "이전 챕터에서 했던 것처럼 기본 글꼴 크기를 정의하여 그림을 더 예쁘게 만들어 보겠습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8d4TH3NbpKzx"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.rc('font', size=14)\n",
        "plt.rc('axes', labelsize=14, titlesize=14)\n",
        "plt.rc('legend', fontsize=14)\n",
        "plt.rc('xtick', labelsize=10)\n",
        "plt.rc('ytick', labelsize=10)\n",
        "\n",
        "import sys\n",
        "# 코랩의 경우 나눔 폰트를 설치합니다.\n",
        "if 'google.colab' in sys.modules:\n",
        "    !sudo apt-get -qq -y install fonts-nanum\n",
        "    import matplotlib.font_manager as fm\n",
        "    font_files = fm.findSystemFonts(fontpaths=['/usr/share/fonts/truetype/nanum'])\n",
        "    for fpath in font_files:\n",
        "        fm.fontManager.addfont(fpath)\n",
        "\n",
        "# 나눔 폰트를 사용합니다.\n",
        "import matplotlib\n",
        "\n",
        "matplotlib.rc('font', family='NanumBarunGothic')\n",
        "matplotlib.rcParams['axes.unicode_minus'] = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcoUIRsvpKzy"
      },
      "source": [
        "그리고 `images/nlp` 폴더를 만들고(아직 존재하지 않는 경우), 이 노트북을 통해 책에 사용할 그림을 고해상도로 저장하는 데 사용되는 `save_fig()` 함수를 정의해 보겠습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQFH5Y9PpKzy"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "IMAGES_PATH = Path() / \"images\" / \"nlp\"\n",
        "IMAGES_PATH.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
        "    path = IMAGES_PATH / f\"{fig_id}.{fig_extension}\"\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    plt.savefig(path, format=fig_extension, dpi=resolution)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTsawKlapKzy"
      },
      "source": [
        "이 챕터는 GPU가 없으면 매우 느려질 수 있으므로 GPU가 있는지 확인하거나 그렇지 않으면 경고를 표시합니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ekxzo6pOpKzy"
      },
      "outputs": [],
      "source": [
        "if not tf.config.list_physical_devices('GPU'):\n",
        "    print(\"GPU가 감지되지 않았습니다. 신경망은 GPU가 없으면 매우 느릴 수 있습니다.\")\n",
        "    if \"google.colab\" in sys.modules:\n",
        "        print(\"런타임 > 런타임 유형 변경으로 이동하여 하드웨어 가속기에서 GPU를 선택합니다.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19z42n_fC45t"
      },
      "source": [
        "# char-RNN을 사용하여 셰익스피어 같은 텍스트 생성하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0N_pUKKC45t"
      },
      "source": [
        "## 훈련 데이터셋 생성하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuX8bfeoC45t"
      },
      "source": [
        "안드레이 카파시의 [char-rnn 프로젝트](https://github.com/karpathy/char-rnn/)에서 셰익스피어 데이터를 다운로드해 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uAN_1XLxC45u"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "shakespeare_url = \"https://homl.info/shakespeare\"  # 단축 URL\n",
        "filepath = tf.keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\n",
        "with open(filepath) as f:\n",
        "    shakespeare_text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbhZwby9C45u"
      },
      "outputs": [],
      "source": [
        "# 추가 코드 - 짧은 텍스트 샘플을 표시합니다.\n",
        "print(shakespeare_text[:80])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPcfVQbNC45u"
      },
      "outputs": [],
      "source": [
        "# 추가 코드 - 39개의 고유 문자를 모두 표시합니다(소문자로 변환 후).\n",
        "\"\".join(sorted(set(shakespeare_text.lower())))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCOCJWIIC45v"
      },
      "outputs": [],
      "source": [
        "text_vec_layer = tf.keras.layers.TextVectorization(split=\"character\",\n",
        "                                                   standardize=\"lower\")\n",
        "text_vec_layer.adapt([shakespeare_text])\n",
        "encoded = text_vec_layer([shakespeare_text])[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L7SF571dC45v"
      },
      "outputs": [],
      "source": [
        "encoded -= 2  # 토큰 0(패딩)과 1(알 수 없음)을 드롭하는데, 이 토큰은 사용하지 않습니다.\n",
        "n_tokens = text_vec_layer.vocabulary_size() - 2  # 고유 문자 수 = 39\n",
        "dataset_size = len(encoded)  # 총 문자 수 = 1,115,394"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VVI2turfC45v"
      },
      "outputs": [],
      "source": [
        "n_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vD5Y8YsDC45v"
      },
      "outputs": [],
      "source": [
        "dataset_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xmj7G74fC45v"
      },
      "outputs": [],
      "source": [
        "def to_dataset(sequence, length, shuffle=False, seed=None, batch_size=32):\n",
        "    ds = tf.data.Dataset.from_tensor_slices(sequence)\n",
        "    ds = ds.window(length + 1, shift=1, drop_remainder=True)\n",
        "    ds = ds.flat_map(lambda window_ds: window_ds.batch(length + 1))\n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(100_000, seed=seed)\n",
        "    ds = ds.batch(batch_size)\n",
        "    return ds.map(lambda window: (window[:, :-1], window[:, 1:])).prefetch(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZGQxvBcoC45v"
      },
      "outputs": [],
      "source": [
        "# 추가 코드 - to_dataset()을 사용하는 간단한 예제\n",
        "# 이 데이터셋에는 하나의 샘플만 있습니다. 입력은 \"to b\"이고 출력은 \"o be\"입니다.\n",
        "list(to_dataset(text_vec_layer([\"To be\"])[0], length=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCJc9kWZC45w"
      },
      "outputs": [],
      "source": [
        "length = 100\n",
        "tf.random.set_seed(42)\n",
        "train_set = to_dataset(encoded[:1_000_000], length=length, shuffle=True,\n",
        "                       seed=42)\n",
        "valid_set = to_dataset(encoded[1_000_000:1_060_000], length=length)\n",
        "test_set = to_dataset(encoded[1_060_000:], length=length)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZM51bj5eC45w"
      },
      "source": [
        "## Char-RNN 모델 구축 및 훈련하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtwgJIltC45w"
      },
      "source": [
        "**경고**: 다음 코드는 GPU에 따라 실행하는 데 1~2시간이 걸릴 수 있습니다. GPU가 없는 경우 24시간 이상 걸릴 수 있습니다. 기다리지 않으려면 다음 두 코드 셀을 건너뛰고 아래 코드를 실행하여 사전 학습된 모델을 다운로드하세요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0qsq8dcC45w"
      },
      "source": [
        "**참고**: (GPU가 있는 경우) `GRU` 클래스는 다음 매개변수의 기본값을 사용할 때 cuDNN 가속을 사용합니다: `activation`, `recurrent_activation`, `recurrent_dropout`, `unroll`, `use_bias`, `reset_after`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJEzwJ6vC45w"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)  # 추가 코드 - CPU에서 재현성 보장\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=n_tokens, output_dim=16),\n",
        "    tf.keras.layers.GRU(128, return_sequences=True),\n",
        "    tf.keras.layers.Dense(n_tokens, activation=\"softmax\")\n",
        "])\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model_ckpt = tf.keras.callbacks.ModelCheckpoint(\n",
        "    \"my_shakespeare_model\", monitor=\"val_accuracy\", save_best_only=True)\n",
        "history = model.fit(train_set, validation_data=valid_set, epochs=10,\n",
        "                    callbacks=[model_ckpt])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sUjgq641C45w"
      },
      "outputs": [],
      "source": [
        "shakespeare_model = tf.keras.Sequential([\n",
        "    text_vec_layer,\n",
        "    tf.keras.layers.Lambda(lambda X: X - 2),  # <PAD>나 <UNK> 토큰 없음\n",
        "    model\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUBE7wDtC45w"
      },
      "source": [
        "훈련이 완료될 때까지 기다리기 싫으시다면 모델을 미리 훈련해 두었습니다. 다음 코드에서 다운로드할 수 있습니다. 위에서 학습된 모델 대신 이 모델을 사용하려면 마지막 줄의 주석 처리를 해제하세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0LZE1rbYC45x"
      },
      "outputs": [],
      "source": [
        "# 추가 코드 - 사전 훈련된 모델 다운로드\n",
        "url = \"https://github.com/ageron/data/raw/main/shakespeare_model.tgz\"\n",
        "path = tf.keras.utils.get_file(\"shakespeare_model.tgz\", url, extract=True)\n",
        "model_path = Path(path).with_name(\"shakespeare_model\")\n",
        "shakespeare_model = tf.keras.models.load_model(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kl8RbP0sC45x"
      },
      "outputs": [],
      "source": [
        "y_proba = shakespeare_model.predict([\"To be or not to b\"])[0, -1]\n",
        "y_pred = tf.argmax(y_proba)  # 가장 가능성이 높은 문자 ID 선택\n",
        "text_vec_layer.get_vocabulary()[y_pred + 2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEQn7SWOC45x"
      },
      "source": [
        "## 가짜 셰익스피어 텍스트 생성하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iYshxcp-C45x"
      },
      "outputs": [],
      "source": [
        "log_probas = tf.math.log([[0.5, 0.4, 0.1]])  # 확률 = 50%, 40%, 10%\n",
        "tf.random.set_seed(42)\n",
        "tf.random.categorical(log_probas, num_samples=8)  # 샘플 8개를 뽑습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4RE41r8C45x"
      },
      "outputs": [],
      "source": [
        "def next_char(text, temperature=1):\n",
        "    y_proba = shakespeare_model.predict([text])[0, -1:]\n",
        "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
        "    char_id = tf.random.categorical(rescaled_logits, num_samples=1)[0, 0]\n",
        "    return text_vec_layer.get_vocabulary()[char_id + 2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BVKVaInnC45x"
      },
      "outputs": [],
      "source": [
        "def extend_text(text, n_chars=50, temperature=1):\n",
        "    for _ in range(n_chars):\n",
        "        text += next_char(text, temperature)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQKB4HDuC45x"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)  # 추가 코드 - CPU에서 재현성 보장"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "itJfQNjYC45y"
      },
      "outputs": [],
      "source": [
        "print(extend_text(\"To be or not to be\", temperature=0.01))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s70j8ddxC45y"
      },
      "outputs": [],
      "source": [
        "print(extend_text(\"To be or not to be\", temperature=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2fjL8A9TC45y"
      },
      "outputs": [],
      "source": [
        "print(extend_text(\"To be or not to be\", temperature=100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYzmlWapC45y"
      },
      "source": [
        "## 상태가 있는 RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OW48BH49C45y"
      },
      "outputs": [],
      "source": [
        "def to_dataset_for_stateful_rnn(sequence, length):\n",
        "    ds = tf.data.Dataset.from_tensor_slices(sequence)\n",
        "    ds = ds.window(length + 1, shift=length, drop_remainder=True)\n",
        "    ds = ds.flat_map(lambda window: window.batch(length + 1)).batch(1)\n",
        "    return ds.map(lambda window: (window[:, :-1], window[:, 1:])).prefetch(1)\n",
        "\n",
        "stateful_train_set = to_dataset_for_stateful_rnn(encoded[:1_000_000], length)\n",
        "stateful_valid_set = to_dataset_for_stateful_rnn(encoded[1_000_000:1_060_000],\n",
        "                                                 length)\n",
        "stateful_test_set = to_dataset_for_stateful_rnn(encoded[1_060_000:], length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55ixV5oIC45y"
      },
      "outputs": [],
      "source": [
        "# 추가 코드 - to_dataset_for_stateful_rnn()을 사용한 간단한 예제\n",
        "list(to_dataset_for_stateful_rnn(tf.range(10), 3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBgLUp5mC45y"
      },
      "source": [
        "배치당 두 개 이상의 윈도가 있다면 `to_dataset_for_stateful_rnn()` 대신 `to_batched_dataset_for_stateful_rnn()` 함수를 사용할 수 있습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fphrw1mvC45y"
      },
      "outputs": [],
      "source": [
        "# 추가 코드 - 상태가 있는 RNN을 위해 배치 데이터셋을 준비하는 한 가지 방법을 보여줍니다.\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def to_non_overlapping_windows(sequence, length):\n",
        "    ds = tf.data.Dataset.from_tensor_slices(sequence)\n",
        "    ds = ds.window(length + 1, shift=length, drop_remainder=True)\n",
        "    return ds.flat_map(lambda window: window.batch(length + 1))\n",
        "\n",
        "def to_batched_dataset_for_stateful_rnn(sequence, length, batch_size=32):\n",
        "    parts = np.array_split(sequence, batch_size)\n",
        "    datasets = tuple(to_non_overlapping_windows(part, length) for part in parts)\n",
        "    ds = tf.data.Dataset.zip(datasets).map(lambda *windows: tf.stack(windows))\n",
        "    return ds.map(lambda window: (window[:, :-1], window[:, 1:])).prefetch(1)\n",
        "\n",
        "list(to_batched_dataset_for_stateful_rnn(tf.range(20), length=3, batch_size=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18lvduzKC45z"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)  # 추가 코드 - CPU에서 재현성 보장\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=n_tokens, output_dim=16,\n",
        "                              batch_input_shape=[1, None]),\n",
        "    tf.keras.layers.GRU(128, return_sequences=True, stateful=True),\n",
        "    tf.keras.layers.Dense(n_tokens, activation=\"softmax\")\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AdzjBI3WC45z"
      },
      "outputs": [],
      "source": [
        "class ResetStatesCallback(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_begin(self, epoch, logs):\n",
        "        self.model.reset_states()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XzXh37RC45z"
      },
      "outputs": [],
      "source": [
        "# 추가 코드 - 다른 디렉터리를 사용하여 체크포인트를 저장합니다.\n",
        "model_ckpt = tf.keras.callbacks.ModelCheckpoint(\n",
        "    \"my_stateful_shakespeare_model\",\n",
        "    monitor=\"val_accuracy\",\n",
        "    save_best_only=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwCT5BAMC45z"
      },
      "source": [
        "**경고**: 다음 셀을 실행하는 데 시간이 걸릴 수 있습니다(GPU를 사용하지 않는 경우 1시간 정도 소요될 수 있음)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FcgJ7-qHC45z"
      },
      "outputs": [],
      "source": [
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n",
        "              metrics=[\"accuracy\"])\n",
        "history = model.fit(stateful_train_set, validation_data=stateful_valid_set,\n",
        "                    epochs=10, callbacks=[ResetStatesCallback(), model_ckpt])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVCoCKLXC45z"
      },
      "source": [
        "**추가 자료: 상태가 있는 RNN을 상태가 없는 RNN으로 변환하여 사용하기**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPC6SI2NC45z"
      },
      "source": [
        "다른 배치 크기로 모델을 사용하려면 상태가 없는 모델 복사본을 만들어야 합니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ssf4bcKJC45z"
      },
      "outputs": [],
      "source": [
        "stateless_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=n_tokens, output_dim=16),\n",
        "    tf.keras.layers.GRU(128, return_sequences=True),\n",
        "    tf.keras.layers.Dense(n_tokens, activation=\"softmax\")\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hldkbULC45z"
      },
      "source": [
        "가중치를 설정하려면 먼저 모델을 빌드해야 합니다(가중치가 생성되도록):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BilmycmEC450"
      },
      "outputs": [],
      "source": [
        "stateless_model.build(tf.TensorShape([None, None]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BlxzIqNkC450"
      },
      "outputs": [],
      "source": [
        "stateless_model.set_weights(model.get_weights())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bj0kvecZC450"
      },
      "outputs": [],
      "source": [
        "shakespeare_model = tf.keras.Sequential([\n",
        "    text_vec_layer,\n",
        "    tf.keras.layers.Lambda(lambda X: X - 2),  # <PAD>나 <UNK> 토큰 없음\n",
        "    stateless_model\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0UB2s8INC450"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "print(extend_text(\"to be or not to be\", temperature=0.01))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpEKVsmWC450"
      },
      "source": [
        "# 감성 분석"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jvqu6p-GC450"
      },
      "outputs": [],
      "source": [
        "import tensorflow_datasets as tfds\n",
        "\n",
        "raw_train_set, raw_valid_set, raw_test_set = tfds.load(\n",
        "    name=\"imdb_reviews\",\n",
        "    split=[\"train[:90%]\", \"train[90%:]\", \"test\"],\n",
        "    as_supervised=True\n",
        ")\n",
        "tf.random.set_seed(42)\n",
        "train_set = raw_train_set.shuffle(5000, seed=42).batch(32).prefetch(1)\n",
        "valid_set = raw_valid_set.batch(32).prefetch(1)\n",
        "test_set = raw_test_set.batch(32).prefetch(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGvb_8GHC450"
      },
      "outputs": [],
      "source": [
        "for review, label in raw_train_set.take(4):\n",
        "    print(review.numpy().decode(\"utf-8\")[:200], \"...\")\n",
        "    print(\"레이블:\", label.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oH5xJUJDC450"
      },
      "outputs": [],
      "source": [
        "vocab_size = 1000\n",
        "text_vec_layer = tf.keras.layers.TextVectorization(max_tokens=vocab_size)\n",
        "text_vec_layer.adapt(train_set.map(lambda reviews, labels: reviews))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12uj9IXmC451"
      },
      "source": [
        "**경고**: 다음 셀은 실행하는 데 몇 분 정도 걸리며 패딩 토큰을 마스킹하지 않았기 때문에 모델이 아무것도 학습하지 못할 수 있습니다(다음 섹션의 요점입니다)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E6CJsFzzC451"
      },
      "outputs": [],
      "source": [
        "embed_size = 128\n",
        "tf.random.set_seed(42)\n",
        "model = tf.keras.Sequential([\n",
        "    text_vec_layer,\n",
        "    tf.keras.layers.Embedding(vocab_size, embed_size),\n",
        "    tf.keras.layers.GRU(128),\n",
        "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\",\n",
        "              metrics=[\"accuracy\"])\n",
        "history = model.fit(train_set, validation_data=valid_set, epochs=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9C5BTnbC451"
      },
      "source": [
        "## 마스킹"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqStWXt1C451"
      },
      "source": [
        "**경고**: 다음 셀을 실행하는 데 시간이 걸립니다(GPU를 사용하지 않는 경우 30분 정도 소요될 수 있습니다)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42P82ac7C451"
      },
      "outputs": [],
      "source": [
        "embed_size = 128\n",
        "tf.random.set_seed(42)\n",
        "model = tf.keras.Sequential([\n",
        "    text_vec_layer,\n",
        "    tf.keras.layers.Embedding(vocab_size, embed_size, mask_zero=True),\n",
        "    tf.keras.layers.GRU(128),\n",
        "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\",\n",
        "              metrics=[\"accuracy\"])\n",
        "history = model.fit(train_set, validation_data=valid_set, epochs=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lc04LwxmC451"
      },
      "source": [
        "또는 수동 마스킹을 사용합니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qv_uXWREC451"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)  # 추가 코드 - CPU에서 재현성 보장\n",
        "inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)\n",
        "token_ids = text_vec_layer(inputs)\n",
        "mask = tf.math.not_equal(token_ids, 0)\n",
        "Z = tf.keras.layers.Embedding(vocab_size, embed_size)(token_ids)\n",
        "Z = tf.keras.layers.GRU(128, dropout=0.2)(Z, mask=mask)\n",
        "outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(Z)\n",
        "model = tf.keras.Model(inputs=[inputs], outputs=[outputs])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkkWZbBkC451"
      },
      "source": [
        "**경고**: 다음 셀을 실행하는 데 시간이 걸립니다(GPU를 사용하지 않는 경우 30분 정도 소요될 수 있습니다)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iplBZbMOC451"
      },
      "outputs": [],
      "source": [
        "# 추가 코드 - 평소와 같이 모델을 컴파일하고 훈련합니다.\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\",\n",
        "              metrics=[\"accuracy\"])\n",
        "history = model.fit(train_set, validation_data=valid_set, epochs=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bobIUkMCC451"
      },
      "source": [
        "**추가 자료: 래그드 텐서 사용하기**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eyswunQDC452"
      },
      "outputs": [],
      "source": [
        "text_vec_layer_ragged = tf.keras.layers.TextVectorization(\n",
        "    max_tokens=vocab_size, ragged=True)\n",
        "text_vec_layer_ragged.adapt(train_set.map(lambda reviews, labels: reviews))\n",
        "text_vec_layer_ragged([\"Great movie!\", \"This is DiCaprio's best role.\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gA9L8_b6C452"
      },
      "outputs": [],
      "source": [
        "text_vec_layer([\"Great movie!\", \"This is DiCaprio's best role.\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzBWyTakC452"
      },
      "source": [
        "**경고**: 다음 셀을 실행하는 데 시간이 걸립니다(GPU를 사용하지 않는 경우 30분 정도 소요될 수 있습니다)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gM9VkQddC452"
      },
      "outputs": [],
      "source": [
        "embed_size = 128\n",
        "tf.random.set_seed(42)\n",
        "model = tf.keras.Sequential([\n",
        "    text_vec_layer_ragged,\n",
        "    tf.keras.layers.Embedding(vocab_size, embed_size),\n",
        "    tf.keras.layers.GRU(128),\n",
        "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\",\n",
        "              metrics=[\"accuracy\"])\n",
        "history = model.fit(train_set, validation_data=valid_set, epochs=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-gBuEnOC452"
      },
      "source": [
        "## 사전 훈련된 임베딩 및 언어 모델 재사용하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kccHHpkC452"
      },
      "source": [
        "**경고**: 다음 셀을 실행하는 데 시간이 걸릴 수 있습니다(GPU를 사용하지 않는 경우 1시간 정도 소요될 수 있음)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dvv6wXCEC452"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "os.environ[\"TFHUB_CACHE_DIR\"] = \"my_tfhub_cache\"\n",
        "tf.random.set_seed(42)  # 추가 코드 - CPU에서 재현성 보장\n",
        "model = tf.keras.Sequential([\n",
        "    # trainable=True로 할 경우 코랩에서 메모리 부족 에러가 발생합니다.\n",
        "    hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\",\n",
        "                   trainable=False, dtype=tf.string, input_shape=[]),\n",
        "    tf.keras.layers.Dense(64, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.fit(train_set, validation_data=valid_set, epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4E8G1YRrC452"
      },
      "source": [
        "# 신경망 기계 번역을 위한 인코더-디코더 네트워크"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PaJrhKvTC453"
      },
      "outputs": [],
      "source": [
        "url = \"https://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\"\n",
        "path = tf.keras.utils.get_file(\"spa-eng.zip\", origin=url, cache_dir=\"datasets\",\n",
        "                               extract=True)\n",
        "text = (Path(path).with_name(\"spa-eng\") / \"spa.txt\").read_text()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OmMKDi4fC453"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "text = text.replace(\"¡\", \"\").replace(\"¿\", \"\")\n",
        "pairs = [line.split(\"\\t\") for line in text.splitlines()]\n",
        "np.random.seed(42)  # 추가 코드 - CPU에서 재현성 보장\n",
        "np.random.shuffle(pairs)\n",
        "sentences_en, sentences_es = zip(*pairs)  # 쌍을 2개의 리스트로 분리합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ud_jKZDaC453"
      },
      "outputs": [],
      "source": [
        "for i in range(3):\n",
        "    print(sentences_en[i], \"=>\", sentences_es[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WAvOGf7YC453"
      },
      "outputs": [],
      "source": [
        "vocab_size = 1000\n",
        "max_length = 50\n",
        "text_vec_layer_en = tf.keras.layers.TextVectorization(\n",
        "    vocab_size, output_sequence_length=max_length)\n",
        "text_vec_layer_es = tf.keras.layers.TextVectorization(\n",
        "    vocab_size, output_sequence_length=max_length)\n",
        "text_vec_layer_en.adapt(sentences_en)\n",
        "text_vec_layer_es.adapt([f\"startofseq {s} endofseq\" for s in sentences_es])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSMQbPbAC453"
      },
      "outputs": [],
      "source": [
        "text_vec_layer_en.get_vocabulary()[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9tHrSKlhC453"
      },
      "outputs": [],
      "source": [
        "text_vec_layer_es.get_vocabulary()[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "relGSfGWC453"
      },
      "outputs": [],
      "source": [
        "X_train = tf.constant(sentences_en[:100_000])\n",
        "X_valid = tf.constant(sentences_en[100_000:])\n",
        "X_train_dec = tf.constant([f\"startofseq {s}\" for s in sentences_es[:100_000]])\n",
        "X_valid_dec = tf.constant([f\"startofseq {s}\" for s in sentences_es[100_000:]])\n",
        "Y_train = text_vec_layer_es([f\"{s} endofseq\" for s in sentences_es[:100_000]])\n",
        "Y_valid = text_vec_layer_es([f\"{s} endofseq\" for s in sentences_es[100_000:]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HLwrDO5HC453"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)  # 추가 코드 - CPU에서 재현성 보장\n",
        "encoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)\n",
        "decoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQ-1elyzC453"
      },
      "outputs": [],
      "source": [
        "embed_size = 128\n",
        "encoder_input_ids = text_vec_layer_en(encoder_inputs)\n",
        "decoder_input_ids = text_vec_layer_es(decoder_inputs)\n",
        "encoder_embedding_layer = tf.keras.layers.Embedding(vocab_size, embed_size,\n",
        "                                                    mask_zero=True)\n",
        "decoder_embedding_layer = tf.keras.layers.Embedding(vocab_size, embed_size,\n",
        "                                                    mask_zero=True)\n",
        "encoder_embeddings = encoder_embedding_layer(encoder_input_ids)\n",
        "decoder_embeddings = decoder_embedding_layer(decoder_input_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7HuTQgTEC454"
      },
      "outputs": [],
      "source": [
        "encoder = tf.keras.layers.LSTM(512, return_state=True)\n",
        "encoder_outputs, *encoder_state = encoder(encoder_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vRsCxv7GC454"
      },
      "outputs": [],
      "source": [
        "decoder = tf.keras.layers.LSTM(512, return_sequences=True)\n",
        "decoder_outputs = decoder(decoder_embeddings, initial_state=encoder_state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oht3tTSrC454"
      },
      "outputs": [],
      "source": [
        "output_layer = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")\n",
        "Y_proba = output_layer(decoder_outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ghwg8FJbC454"
      },
      "source": [
        "**경고**: 다음 셀을 실행하는 데 시간이 걸릴 수 있습니다(GPU를 사용하지 않는 경우 몇 시간이 걸릴 수 있습니다)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0V-KsbIKC454"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Model(inputs=[encoder_inputs, decoder_inputs],\n",
        "                       outputs=[Y_proba])\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.fit((X_train, X_train_dec), Y_train, epochs=10,\n",
        "          validation_data=((X_valid, X_valid_dec), Y_valid))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45lY6t8-C454"
      },
      "outputs": [],
      "source": [
        "def translate(sentence_en):\n",
        "    translation = \"\"\n",
        "    for word_idx in range(max_length):\n",
        "        X = np.array([sentence_en])  # encoder input\n",
        "        X_dec = np.array([\"startofseq \" + translation])  # decoder input\n",
        "        y_proba = model.predict((X, X_dec))[0, word_idx]  # last token's probas\n",
        "        predicted_word_id = np.argmax(y_proba)\n",
        "        predicted_word = text_vec_layer_es.get_vocabulary()[predicted_word_id]\n",
        "        if predicted_word == \"endofseq\":\n",
        "            break\n",
        "        translation += \" \" + predicted_word\n",
        "    return translation.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3c4MuFBIC454"
      },
      "outputs": [],
      "source": [
        "translate(\"I like soccer\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqIJLD6AC454"
      },
      "source": [
        "멋지네요! 그러나 이 모델은 긴 문장을 처리하는 데 어려움을 겪습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qzHziRN2C454"
      },
      "outputs": [],
      "source": [
        "translate(\"I like soccer and also going to the beach\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZ7XzRlPC454"
      },
      "source": [
        "## 양방향 RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1s_45zNC455"
      },
      "source": [
        "양방향 순환 층을 만들려면 일반 순환 층을 `Bidirectional` 층으로 감싸면 됩니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OTGo3rMJC455"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)  # 추가 코드 - CPU에서 재현성 보장\n",
        "encoder = tf.keras.layers.Bidirectional(\n",
        "    tf.keras.layers.LSTM(256, return_state=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Fdt9Wt2C455"
      },
      "outputs": [],
      "source": [
        "encoder_outputs, *encoder_state = encoder(encoder_embeddings)\n",
        "encoder_state = [tf.concat(encoder_state[::2], axis=-1),  # 단기 상태 (0 & 2)\n",
        "                 tf.concat(encoder_state[1::2], axis=-1)]  # 장기 상태 (1 & 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6USBcH8DC455"
      },
      "source": [
        "**경고**: 다음 셀을 실행하는 데 시간이 걸릴 수 있습니다(GPU를 사용하지 않는 경우 몇 시간이 걸릴 수 있습니다)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5kKFcAuC455"
      },
      "outputs": [],
      "source": [
        "# 추가 코드 - 모델을 완성하고 학습시킵니다.\n",
        "decoder = tf.keras.layers.LSTM(512, return_sequences=True)\n",
        "decoder_outputs = decoder(decoder_embeddings, initial_state=encoder_state)\n",
        "output_layer = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")\n",
        "Y_proba = output_layer(decoder_outputs)\n",
        "model = tf.keras.Model(inputs=[encoder_inputs, decoder_inputs],\n",
        "                       outputs=[Y_proba])\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.fit((X_train, X_train_dec), Y_train, epochs=10,\n",
        "          validation_data=((X_valid, X_valid_dec), Y_valid))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wyS-yKphC455"
      },
      "outputs": [],
      "source": [
        "translate(\"I like soccer\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZX7PZXKDC455"
      },
      "source": [
        "## 빔 검색"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yjh98h5gC455"
      },
      "source": [
        "이것은 빔 검색의 매우 기본적인 구현입니다. 읽기 쉽고 이해하기 쉽게 만들려고 노력했지만 속도에 최적화되지는 않았습니다! 이 함수는 먼저 모델을 사용하여 번역을 시작하기 위해 상위 _k_ 단어를 찾습니다(여기서 _k_는 빔 너비). 상위 _k_ 번역 각각에 대해 해당 번역에 추가할 수 있는 모든 가능한 단어의 조건부 확률을 평가합니다. 이러한 확장 번역과 해당 확률이 후보 목록에 추가됩니다. 모든 상위 _k_ 번역과 번역을 완성할 수 있는 모든 단어를 검토한 후에는 확률이 가장 높은 상위 _k_ 후보만 유지하고 모든 번역이 EOS 토큰으로 완료될 때까지 반복해서 반복합니다. 그런 다음 상위 번역이 반환됩니다(해당 EOS 토큰을 제거한 후).\n",
        "\n",
        "* 참고: p(S)가 문장 S의 확률이고 p(W|S)가 번역이 S로 시작한다는 가정 하에 단어 W의 조건부 확률인 경우, 문장 S' = concat(S, W)의 확률은 p(S') = p(S) * p(W|S)입니다. 단어를 더 추가할수록 확률은 점점 더 작아집니다. 너무 작아져 부동 소수점 정밀도 오류가 발생할 수 있는 위험을 피하기 위해 이 함수는 확률 대신 로그 확률을 추적합니다. log(a\\*b) = log(a) + log(b), 따라서 log(p(S')) = log(p(S)) + log(p(W|S))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RyX3_XjhC455"
      },
      "outputs": [],
      "source": [
        "# 추가 코드 - 빔 검색의 기본 구현\n",
        "\n",
        "def beam_search(sentence_en, beam_width, verbose=False):\n",
        "    X = np.array([sentence_en])  # 인코더 입력\n",
        "    X_dec = np.array([\"startofseq\"])  # 디코더 입력\n",
        "    y_proba = model.predict((X, X_dec))[0, 0]  # 첫 번째 토큰의 확률\n",
        "    top_k = tf.math.top_k(y_proba, k=beam_width)\n",
        "    top_translations = [  # 촤상의 (log_proba, translation) 리스트\n",
        "        (np.log(word_proba), text_vec_layer_es.get_vocabulary()[word_id])\n",
        "        for word_proba, word_id in zip(top_k.values, top_k.indices)\n",
        "    ]\n",
        "\n",
        "    # 추가 코드 - verbose 모드에서 상위 첫 단어를 표시합니다.\n",
        "    if verbose:\n",
        "        print(\"상위 첫 단어:\", top_translations)\n",
        "\n",
        "    for idx in range(1, max_length):\n",
        "        candidates = []\n",
        "        for log_proba, translation in top_translations:\n",
        "            if translation.endswith(\"endofseq\"):\n",
        "                candidates.append((log_proba, translation))\n",
        "                continue  # 번역이 완료되었으므로 번역을 이어가지 않습니다.\n",
        "            X = np.array([sentence_en])  # 인코더 입력\n",
        "            X_dec = np.array([\"startofseq \" + translation])  # 디코더 입력\n",
        "            y_proba = model.predict((X, X_dec))[0, idx]  # 마지막 토큰의 확률\n",
        "            for word_id, word_proba in enumerate(y_proba):\n",
        "                word = text_vec_layer_es.get_vocabulary()[word_id]\n",
        "                candidates.append((log_proba + np.log(word_proba),\n",
        "                                   f\"{translation} {word}\"))\n",
        "        top_translations = sorted(candidates, reverse=True)[:beam_width]\n",
        "\n",
        "        # 추가 코드 - verbose 모드의 경우 지금까지의 최상의 번역을 출력합니다.\n",
        "        if verbose:\n",
        "            print(\"지금까지 최상의 번역:\", top_translations)\n",
        "\n",
        "        if all([tr.endswith(\"endofseq\") for _, tr in top_translations]):\n",
        "            return top_translations[0][1].replace(\"endofseq\", \"\").strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9LirxQMpC456"
      },
      "outputs": [],
      "source": [
        "# 추가 코드 - 모델이 어떻게 오류를 발생시키는지 보여줍니다.\n",
        "sentence_en = \"I love cats and dogs\"\n",
        "translate(sentence_en)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPXxMOXdC456"
      },
      "outputs": [],
      "source": [
        "# 추가 코드 - 빔 검색이 어떻게 도움이 되는지 보여줍니다.\n",
        "beam_search(sentence_en, beam_width=3, verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFqpL8BqC456"
      },
      "source": [
        "빔 검색에서 찾은 상위 3개 문장에 올바른 번역이 있지만 첫 번째 번역은 아닙니다. 작은 어휘를 사용하기 때문에 \\[UNK] 토큰이 꽤 자주 사용되므로 페널티를 줄 수 있습니다(예를 들어, 빔 검색 함수에서 이 토큰의 확률을 2로 나눕니다.): 이렇게 하면 빔 검색이 이 토큰을 너무 많이 사용하지 않게 됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUCUlOe-C456"
      },
      "source": [
        "# 어텐션 메커니즘"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIsLqkm8C456"
      },
      "source": [
        "모든 인코더의 출력을 `Attention` 층에 공급해야 하므로 인코더에 `return_sequences=True`를 추가해야 합니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-rNRZIkC456"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)  # 추가 코드 - CPU에서 재현성 보장\n",
        "encoder = tf.keras.layers.Bidirectional(\n",
        "    tf.keras.layers.LSTM(256, return_sequences=True, return_state=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JhIdaM3xC456"
      },
      "outputs": [],
      "source": [
        "# 추가 코드 - 모델의 이 부분은 이전과 완전히 동일합니다.\n",
        "encoder_outputs, *encoder_state = encoder(encoder_embeddings)\n",
        "encoder_state = [tf.concat(encoder_state[::2], axis=-1),  # 단기 (0 & 2)\n",
        "                 tf.concat(encoder_state[1::2], axis=-1)]  # 장기 (1 & 3)\n",
        "decoder = tf.keras.layers.LSTM(512, return_sequences=True)\n",
        "decoder_outputs = decoder(decoder_embeddings, initial_state=encoder_state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygB3hXHcC456"
      },
      "source": [
        "마지막으로 'Attention' 층과 출력 층을 추가해 보겠습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5HwprzWC456"
      },
      "outputs": [],
      "source": [
        "attention_layer = tf.keras.layers.Attention()\n",
        "attention_outputs = attention_layer([decoder_outputs, encoder_outputs])\n",
        "output_layer = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")\n",
        "Y_proba = output_layer(attention_outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrI_qChjC456"
      },
      "source": [
        "**경고**: 다음 셀을 실행하는 데 시간이 걸릴 수 있습니다(GPU를 사용하지 않는 경우 몇 시간이 걸릴 수 있습니다)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "okXEBXhUC456"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Model(inputs=[encoder_inputs, decoder_inputs],\n",
        "                       outputs=[Y_proba])\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.fit((X_train, X_train_dec), Y_train, epochs=10,\n",
        "          validation_data=((X_valid, X_valid_dec), Y_valid))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vXvP6O28C457"
      },
      "outputs": [],
      "source": [
        "translate(\"I like soccer and also going to the beach\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ARGoK9JC457"
      },
      "outputs": [],
      "source": [
        "beam_search(\"I like soccer and also going to the beach\", beam_width=3,\n",
        "            verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJIqmnc6C457"
      },
      "source": [
        "## 트랜스포머 구조: 어텐션이 필요한 전부다\n",
        "### 위치 인코딩"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LAnQQ7o5C457"
      },
      "outputs": [],
      "source": [
        "max_length = 50  # 전체 훈련 세트에 있는 최대 길이\n",
        "embed_size = 128\n",
        "tf.random.set_seed(42)  # 추가 코드 - CPU에서 재현성 보장\n",
        "pos_embed_layer = tf.keras.layers.Embedding(max_length, embed_size)\n",
        "batch_max_len_enc = tf.shape(encoder_embeddings)[1]\n",
        "encoder_in = encoder_embeddings + pos_embed_layer(tf.range(batch_max_len_enc))\n",
        "batch_max_len_dec = tf.shape(decoder_embeddings)[1]\n",
        "decoder_in = decoder_embeddings + pos_embed_layer(tf.range(batch_max_len_dec))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjQlUHd5C457"
      },
      "source": [
        "또는 훈련하지 않는 고정 위치 인코딩을 사용할 수도 있습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yElNnWh8C457"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(tf.keras.layers.Layer):\n",
        "    def __init__(self, max_length, embed_size, dtype=tf.float32, **kwargs):\n",
        "        super().__init__(dtype=dtype, **kwargs)\n",
        "        assert embed_size % 2 == 0, \"embed_size must be even\"\n",
        "        p, i = np.meshgrid(np.arange(max_length),\n",
        "                           2 * np.arange(embed_size // 2))\n",
        "        pos_emb = np.empty((1, max_length, embed_size))\n",
        "        pos_emb[0, :, ::2] = np.sin(p / 10_000 ** (i / embed_size)).T\n",
        "        pos_emb[0, :, 1::2] = np.cos(p / 10_000 ** (i / embed_size)).T\n",
        "        self.pos_encodings = tf.constant(pos_emb.astype(self.dtype))\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs):\n",
        "        batch_max_length = tf.shape(inputs)[1]\n",
        "        return inputs + self.pos_encodings[:, :batch_max_length]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7dOkPc5C457"
      },
      "outputs": [],
      "source": [
        "pos_embed_layer = PositionalEncoding(max_length, embed_size)\n",
        "encoder_in = pos_embed_layer(encoder_embeddings)\n",
        "decoder_in = pos_embed_layer(decoder_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YvbES7RFC457"
      },
      "outputs": [],
      "source": [
        "# 추가 코드\n",
        "figure_max_length = 201\n",
        "figure_embed_size = 512\n",
        "pos_emb = PositionalEncoding(figure_max_length, figure_embed_size)\n",
        "zeros = np.zeros((1, figure_max_length, figure_embed_size), np.float32)\n",
        "P = pos_emb(zeros)[0].numpy()\n",
        "i1, i2, crop_i = 100, 101, 150\n",
        "p1, p2, p3 = 22, 60, 35\n",
        "fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, sharex=True, figsize=(9, 5))\n",
        "ax1.plot([p1, p1], [-1, 1], \"k--\", label=\"$p = {}$\".format(p1))\n",
        "ax1.plot([p2, p2], [-1, 1], \"k--\", label=\"$p = {}$\".format(p2), alpha=0.5)\n",
        "ax1.plot(p3, P[p3, i1], \"bx\", label=\"$p = {}$\".format(p3))\n",
        "ax1.plot(P[:,i1], \"b-\", label=\"$i = {}$\".format(i1))\n",
        "ax1.plot(P[:,i2], \"r-\", label=\"$i = {}$\".format(i2))\n",
        "ax1.plot([p1, p2], [P[p1, i1], P[p2, i1]], \"bo\")\n",
        "ax1.plot([p1, p2], [P[p1, i2], P[p2, i2]], \"ro\")\n",
        "ax1.legend(loc=\"center right\", fontsize=14, framealpha=0.95)\n",
        "ax1.set_ylabel(\"$P_{(p,i)}$\", rotation=0, fontsize=16)\n",
        "ax1.grid(True, alpha=0.3)\n",
        "ax1.hlines(0, 0, figure_max_length - 1, color=\"k\", linewidth=1, alpha=0.3)\n",
        "ax1.axis([0, figure_max_length - 1, -1, 1])\n",
        "ax2.imshow(P.T[:crop_i], cmap=\"gray\", interpolation=\"bilinear\", aspect=\"auto\")\n",
        "ax2.hlines(i1, 0, figure_max_length - 1, color=\"b\", linewidth=3)\n",
        "cheat = 2  # need to raise the red line a bit, or else it hides the blue one\n",
        "ax2.hlines(i2+cheat, 0, figure_max_length - 1, color=\"r\", linewidth=3)\n",
        "ax2.plot([p1, p1], [0, crop_i], \"k--\")\n",
        "ax2.plot([p2, p2], [0, crop_i], \"k--\", alpha=0.5)\n",
        "ax2.plot([p1, p2], [i2+cheat, i2+cheat], \"ro\")\n",
        "ax2.plot([p1, p2], [i1, i1], \"bo\")\n",
        "ax2.axis([0, figure_max_length - 1, 0, crop_i])\n",
        "ax2.set_xlabel(\"$p$\", fontsize=16)\n",
        "ax2.set_ylabel(\"$i$\", rotation=0, fontsize=16)\n",
        "save_fig(\"positional_embedding_plot\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "korxcX1rC458"
      },
      "source": [
        "### 멀티 헤드 어텐션"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9lSdmUzyC458"
      },
      "outputs": [],
      "source": [
        "N = 2  # 원본 구조는 6\n",
        "num_heads = 8\n",
        "dropout_rate = 0.1\n",
        "n_units = 128  # 피드 포워드 블록의 첫 번째 Dense 층의 유닛 개수\n",
        "encoder_pad_mask = tf.math.not_equal(encoder_input_ids, 0)[:, tf.newaxis]\n",
        "Z = encoder_in\n",
        "for _ in range(N):\n",
        "    skip = Z\n",
        "    attn_layer = tf.keras.layers.MultiHeadAttention(\n",
        "        num_heads=num_heads, key_dim=embed_size, dropout=dropout_rate)\n",
        "    Z = attn_layer(Z, value=Z, attention_mask=encoder_pad_mask)\n",
        "    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))\n",
        "    skip = Z\n",
        "    Z = tf.keras.layers.Dense(n_units, activation=\"relu\")(Z)\n",
        "    Z = tf.keras.layers.Dense(embed_size)(Z)\n",
        "    Z = tf.keras.layers.Dropout(dropout_rate)(Z)\n",
        "    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3phVF7mZC458"
      },
      "outputs": [],
      "source": [
        "decoder_pad_mask = tf.math.not_equal(decoder_input_ids, 0)[:, tf.newaxis]\n",
        "causal_mask = tf.linalg.band_part(  # 하삼각행렬을 생성합니다.\n",
        "    tf.ones((batch_max_len_dec, batch_max_len_dec), tf.bool), -1, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57Z0AJeqC458"
      },
      "outputs": [],
      "source": [
        "encoder_outputs = Z  # 인코더의 최종 출력을 저장해 보겠습니다.\n",
        "Z = decoder_in  # 디코더는 자체 입력으로 시작합니다.\n",
        "for _ in range(N):\n",
        "    skip = Z\n",
        "    attn_layer = tf.keras.layers.MultiHeadAttention(\n",
        "        num_heads=num_heads, key_dim=embed_size, dropout=dropout_rate)\n",
        "    Z = attn_layer(Z, value=Z, attention_mask=causal_mask & decoder_pad_mask)\n",
        "    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))\n",
        "    skip = Z\n",
        "    attn_layer = tf.keras.layers.MultiHeadAttention(\n",
        "        num_heads=num_heads, key_dim=embed_size, dropout=dropout_rate)\n",
        "    Z = attn_layer(Z, value=encoder_outputs, attention_mask=encoder_pad_mask)\n",
        "    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))\n",
        "    skip = Z\n",
        "    Z = tf.keras.layers.Dense(n_units, activation=\"relu\")(Z)\n",
        "    Z = tf.keras.layers.Dense(embed_size)(Z)\n",
        "    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycKiICgIC458"
      },
      "source": [
        "**경고**: 다음 셀을 실행하는 데 시간이 걸릴 수 있습니다(GPU를 사용하지 않는 경우 2~3시간 정도 소요될 수 있습니다)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__MZCUCBC458"
      },
      "outputs": [],
      "source": [
        "Y_proba = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")(Z)\n",
        "model = tf.keras.Model(inputs=[encoder_inputs, decoder_inputs],\n",
        "                       outputs=[Y_proba])\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.fit((X_train, X_train_dec), Y_train, epochs=10,\n",
        "          validation_data=((X_valid, X_valid_dec), Y_valid))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-clYUdADC458"
      },
      "outputs": [],
      "source": [
        "translate(\"I like soccer and also going to the beach\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tD7fD4YBC458"
      },
      "source": [
        "# 허깅 페이스"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ra-US0kjC458"
      },
      "source": [
        "코랩에서 실행하는 경우 트랜스포머스 및 데이터셋 라이브러리를 설치합니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rCyu5LjqC459"
      },
      "outputs": [],
      "source": [
        "if \"google.colab\" in sys.modules:\n",
        "    %pip install -q -U transformers\n",
        "    %pip install -q -U datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ojXkbEn6C459"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"sentiment-analysis\")  # 다른 많은 작업을 사용할 수 있습니다.\n",
        "result = classifier(\"The actors were very convincing.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDCKXay4C459"
      },
      "source": [
        "모델은 매우 편향적일 수 있습니다. 예를 들어, 훈련 데이터와 사용 방법에 따라 일부 국가를 좋아하거나 싫어할 수 있으므로 신중하게 사용하세요:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCg5-SHUC459"
      },
      "outputs": [],
      "source": [
        "classifier([\"I am from India.\", \"I am from Iraq.\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uUx8IbKuC459"
      },
      "outputs": [],
      "source": [
        "model_name = \"huggingface/distilbert-base-uncased-finetuned-mnli\"\n",
        "classifier_mnli = pipeline(\"text-classification\", model=model_name)\n",
        "classifier_mnli(\"She loves me. [SEP] She loves me not.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LI8riqr5C459"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "my6XeY67C459"
      },
      "outputs": [],
      "source": [
        "token_ids = tokenizer([\"I like soccer. [SEP] We all love soccer!\",\n",
        "                       \"Joe lived for a very long time. [SEP] Joe is old.\"],\n",
        "                      padding=True, return_tensors=\"tf\")\n",
        "token_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DbJtA8zFC459"
      },
      "outputs": [],
      "source": [
        "token_ids = tokenizer([(\"I like soccer.\", \"We all love soccer!\"),\n",
        "                       (\"Joe lived for a very long time.\", \"Joe is old.\")],\n",
        "                      padding=True, return_tensors=\"tf\")\n",
        "token_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A9ug694NC459"
      },
      "outputs": [],
      "source": [
        "outputs = model(token_ids)\n",
        "outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HIlbEUh0C45-"
      },
      "outputs": [],
      "source": [
        "Y_probas = tf.keras.activations.softmax(outputs.logits)\n",
        "Y_probas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SpVg2ouSC45-"
      },
      "outputs": [],
      "source": [
        "Y_pred = tf.argmax(Y_probas, axis=1)\n",
        "Y_pred  # 0 = contradiction, 1 = entailment, 2 = neutral"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lbmfSRt6C45-"
      },
      "outputs": [],
      "source": [
        "sentences = [(\"Sky is blue\", \"Sky is red\"), (\"I love her\", \"She loves me\")]\n",
        "X_train = tokenizer(sentences, padding=True, return_tensors=\"tf\").data\n",
        "y_train = tf.constant([0, 2])  # contradiction, neutral\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "model.compile(loss=loss, optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
        "history = model.fit(X_train, y_train, epochs=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xc3RWZHC45-"
      },
      "source": [
        "# 연습문제 해답"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLDEEspbC45-"
      },
      "source": [
        "## 1. to 7."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPu2UpX_C45-"
      },
      "source": [
        "## 8.\n",
        "_연습문제: 호크라이터와 슈미트후버는 LSTM에 관한 [논문](https://homl.info/93)에서 임베딩된 레버 문법을 사용했습니다. 이는 ‘BPBTSXXVPSEPE’와 같은 문자열을 만드는 인공 문법입니다. 이 주제에 대한 제니 오어의 훌륭한 소개(https://homl.info/108)를 확인해보세요. 특정 임베딩된 레버 문법 하나를 선택하고(제니 오어의 페이지에 있는 것과 같은), 그다음에 문자열이 이 문법을 따르는지 아닌지 구별하는 RNN을 훈련해보세요. 먼저 문법에 맞는 문자열 50%와 그렇지 않은 문자열 50%를 담은 훈련 배치를 생성하는 함수를 만들어야 합니다._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8oV6k3yC45-"
      },
      "source": [
        "먼저 문법에 맞는 문자열을 생성하는 함수가 필요합니다. 이 문법은 각 상태에서 가능한 전이 상태의 리스트입니다. 하나의 전이는 출력할 문자열(또는 생성할 문법)과 다음 상태를 지정합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nx0sOrZXC45_"
      },
      "outputs": [],
      "source": [
        "default_reber_grammar = [\n",
        "    [(\"B\", 1)],           # (state 0) =B=>(state 1)\n",
        "    [(\"T\", 2), (\"P\", 3)], # (state 1) =T=>(state 2) or =P=>(state 3)\n",
        "    [(\"S\", 2), (\"X\", 4)], # (state 2) =S=>(state 2) or =X=>(state 4)\n",
        "    [(\"T\", 3), (\"V\", 5)], # 등등 ...\n",
        "    [(\"X\", 3), (\"S\", 6)],\n",
        "    [(\"P\", 4), (\"V\", 6)],\n",
        "    [(\"E\", None)]]        # (state 6) =E=>(terminal state)\n",
        "\n",
        "embedded_reber_grammar = [\n",
        "    [(\"B\", 1)],\n",
        "    [(\"T\", 2), (\"P\", 3)],\n",
        "    [(default_reber_grammar, 4)],\n",
        "    [(default_reber_grammar, 5)],\n",
        "    [(\"T\", 6)],\n",
        "    [(\"P\", 6)],\n",
        "    [(\"E\", None)]]\n",
        "\n",
        "def generate_string(grammar):\n",
        "    state = 0\n",
        "    output = []\n",
        "    while state is not None:\n",
        "        index = np.random.randint(len(grammar[state]))\n",
        "        production, state = grammar[state][index]\n",
        "        if isinstance(production, list):\n",
        "            production = generate_string(grammar=production)\n",
        "        output.append(production)\n",
        "    return \"\".join(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RI0N2i3WC45_"
      },
      "source": [
        "기본 레버 문법을 기반으로 몇 가지 문자열을 생성해 보겠습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oSNDdGzxC45_"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "for _ in range(25):\n",
        "    print(generate_string(default_reber_grammar), end=\" \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkez1mYGC45_"
      },
      "source": [
        "좋아 보이네요. 이제 임베딩된 레버 문법을 기반으로 몇 가지 문자열을 생성해 보겠습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aO2VwxHQC45_"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "for _ in range(25):\n",
        "    print(generate_string(embedded_reber_grammar), end=\" \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flmVfAElC45_"
      },
      "source": [
        "좋네요, 이제 이 문법을 따르지 않는 문자열을 생성할 함수를 만듭니다. 무작위하게 문자열을 만들 수 있지만 그렇게 하면 너무 문제가 쉬워지므로 대신 문법을 따르는 문자열을 만든 후 하나의 문자만 바꾸어 놓도록 하겠습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ZY6tDtOC45_"
      },
      "outputs": [],
      "source": [
        "POSSIBLE_CHARS = \"BEPSTVX\"\n",
        "\n",
        "def generate_corrupted_string(grammar, chars=POSSIBLE_CHARS):\n",
        "    good_string = generate_string(grammar)\n",
        "    index = np.random.randint(len(good_string))\n",
        "    good_char = good_string[index]\n",
        "    bad_char = np.random.choice(sorted(set(chars) - set(good_char)))\n",
        "    return good_string[:index] + bad_char + good_string[index + 1:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0_BvQc5C46A"
      },
      "source": [
        "잘못된 문자열 몇 개를 만들어 보죠:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHESmc2RC46A"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "for _ in range(25):\n",
        "    print(generate_corrupted_string(embedded_reber_grammar), end=\" \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDZ-yEe1C46A"
      },
      "source": [
        "문자열을 바로 RNN에 주입할 수는 없기 때문에 어떤 식으로든 인코딩해야 합니다. 한 가지 방법은 각 문자를 원-핫 인코딩하는 것입니다. 또 다른 방식은 임베딩을 사용하는 것입니다. 두 번째 방법을 사용해 보겠습니다(문자 개수가 작다면 원-핫 인코딩도 좋은 선택일 것입니다). 임베딩을 위해 각 문자열을 문자 ID의 시퀀스로 바꾸어야 합니다. 가능한 문자 \"BEPSTVX\"의 문자열 인덱스를 사용해 이런 작업을 수행하는 함수를 만들어 보겠습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IkokV-4gC46A"
      },
      "outputs": [],
      "source": [
        "def string_to_ids(s, chars=POSSIBLE_CHARS):\n",
        "    return [chars.index(c) for c in s]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5HUdUSKC46A"
      },
      "outputs": [],
      "source": [
        "string_to_ids(\"BTTTXXVVETE\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdx_-rYUC46A"
      },
      "source": [
        "이제 50%는 올바른 문자열 50%는 잘못된 문자열로 이루어진 데이터셋을 만듭니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IVZOpmlHC46A"
      },
      "outputs": [],
      "source": [
        "def generate_dataset(size):\n",
        "    good_strings = [\n",
        "        string_to_ids(generate_string(embedded_reber_grammar))\n",
        "        for _ in range(size // 2)\n",
        "    ]\n",
        "    bad_strings = [\n",
        "        string_to_ids(generate_corrupted_string(embedded_reber_grammar))\n",
        "        for _ in range(size - size // 2)\n",
        "    ]\n",
        "    all_strings = good_strings + bad_strings\n",
        "    X = tf.ragged.constant(all_strings, ragged_rank=1)\n",
        "    y = np.array([[1.] for _ in range(len(good_strings))] +\n",
        "                 [[0.] for _ in range(len(bad_strings))])\n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2fdot5NC46A"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "X_train, y_train = generate_dataset(10000)\n",
        "X_valid, y_valid = generate_dataset(2000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTI9Yh-_C46A"
      },
      "source": [
        "첫 번째 훈련 샘플을 확인해 보겠습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UviLZYNjC46B"
      },
      "outputs": [],
      "source": [
        "X_train[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RqgWR5ZC46B"
      },
      "source": [
        "어떤 클래스에 속할까요?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fAFbAGpC46B"
      },
      "outputs": [],
      "source": [
        "y_train[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fRBiyoKC46B"
      },
      "source": [
        "완벽합니다! 이제 올바른 문자열을 구분할 RNN을 만들 준비가 되었습니다. 간단한 시퀀스 이진 분류기를 만듭니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zd6OcjicC46B"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "embedding_size = 5\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.InputLayer(input_shape=[None], dtype=tf.int32, ragged=True),\n",
        "    tf.keras.layers.Embedding(input_dim=len(POSSIBLE_CHARS),\n",
        "                              output_dim=embedding_size),\n",
        "    tf.keras.layers.GRU(30),\n",
        "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.02, momentum = 0.95,\n",
        "                                    nesterov=True)\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=optimizer,\n",
        "              metrics=[\"accuracy\"])\n",
        "history = model.fit(X_train, y_train, epochs=20,\n",
        "                    validation_data=(X_valid, y_valid))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpW6UeU4C46B"
      },
      "source": [
        "이제 두 개의 까다로운 문자열로 이 RNN을 테스트해 보죠: 첫 번째는 잘못된 것이고 두 번째는 올바른 것입니다. 이 문자열은 마지막에서 두 번째 글자만 다릅니다. RNN이 이를 맞춘다면 두 번째 문자가 항상 끝에서 두 번째 문자와 같아야 한다는 패턴을 알게 됐다는 것을 의미합니다. 이렇게 하려면 꽤 긴 단기 기억(long short-term memory)이 필요합니다(그래서 GRU 셀을 사용했습니다)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgJZ6rr0C46B"
      },
      "outputs": [],
      "source": [
        "test_strings = [\"BPBTSSSSSSSXXTTVPXVPXTTTTTVVETE\",\n",
        "                \"BPBTSSSSSSSXXTTVPXVPXTTTTTVVEPE\"]\n",
        "X_test = tf.ragged.constant([string_to_ids(s) for s in test_strings], ragged_rank=1)\n",
        "\n",
        "y_proba = model.predict(X_test)\n",
        "print()\n",
        "print(\"레버 문자열일 추정 확률:\")\n",
        "for index, string in enumerate(test_strings):\n",
        "    print(\"{}: {:.2f}%\".format(string, 100 * y_proba[index][0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdffdfhhC46B"
      },
      "source": [
        "쨘! 잘 작동하네요. 이 RNN이 매우 높은 신뢰도로 정확한 답을 냈습니다. :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCmWD4tnC46B"
      },
      "source": [
        "## 9.\n",
        "_연습문제: 날짜 문자열 포맷을 변환하는 인코더-디코더 모델을 훈련하세요(예를 들어, ‘April 22, 2019’에서 ‘2019-04-22’로 바꿉니다)._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ge3AVnYmC46C"
      },
      "source": [
        "먼저 데이터셋을 만들어 보죠. 1000-01-01 ~ 9999-12-31 사이의 랜덤한 날짜를 사용하겠습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-f5t_67C46C"
      },
      "outputs": [],
      "source": [
        "from datetime import date\n",
        "\n",
        "# strftime()의 %B 포맷은 로케일에 의존하기 때문에 사용할 수 있습니다.\n",
        "MONTHS = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\",\n",
        "          \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]\n",
        "\n",
        "def random_dates(n_dates):\n",
        "    min_date = date(1000, 1, 1).toordinal()\n",
        "    max_date = date(9999, 12, 31).toordinal()\n",
        "\n",
        "    ordinals = np.random.randint(max_date - min_date, size=n_dates) + min_date\n",
        "    dates = [date.fromordinal(ordinal) for ordinal in ordinals]\n",
        "\n",
        "    x = [MONTHS[dt.month - 1] + \" \" + dt.strftime(\"%d, %Y\") for dt in dates]\n",
        "    y = [dt.isoformat() for dt in dates]\n",
        "    return x, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaEpKS1oC46C"
      },
      "source": [
        "다음은 입력과 출력 형식에 맞춘 랜덤한 몇 개의 날짜입니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21czij8VC46C"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "n_dates = 3\n",
        "x_example, y_example = random_dates(n_dates)\n",
        "print(\"{:25s}{:25s}\".format(\"Input\", \"Target\"))\n",
        "print(\"-\" * 50)\n",
        "for idx in range(n_dates):\n",
        "    print(\"{:25s}{:25s}\".format(x_example[idx], y_example[idx]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nszUXihGC46C"
      },
      "source": [
        "입력 가능한 전체 문자를 나열해 보죠:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CY7Knl58C46C"
      },
      "outputs": [],
      "source": [
        "INPUT_CHARS = \"\".join(sorted(set(\"\".join(MONTHS) + \"0123456789, \")))\n",
        "INPUT_CHARS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZPlepcgC46C"
      },
      "source": [
        "그리고 다음은 출력 가능한 전체 문자입니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RG7nVJDLC46C"
      },
      "outputs": [],
      "source": [
        "OUTPUT_CHARS = \"0123456789-\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "al_1vGceC46C"
      },
      "source": [
        "이전 연습문제에서처럼 문자열을 문자 ID 리스트로 바꾸는 함수를 작성해 보겠습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMHco9w7C46C"
      },
      "outputs": [],
      "source": [
        "def date_str_to_ids(date_str, chars=INPUT_CHARS):\n",
        "    return [chars.index(c) for c in date_str]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W1ar8s8dC46C"
      },
      "outputs": [],
      "source": [
        "date_str_to_ids(x_example[0], INPUT_CHARS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFrnenuaC46D"
      },
      "outputs": [],
      "source": [
        "date_str_to_ids(y_example[0], OUTPUT_CHARS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VikEI1hmC46D"
      },
      "outputs": [],
      "source": [
        "def prepare_date_strs(date_strs, chars=INPUT_CHARS):\n",
        "    X_ids = [date_str_to_ids(dt, chars) for dt in date_strs]\n",
        "    X = tf.ragged.constant(X_ids, ragged_rank=1)\n",
        "    return (X + 1).to_tensor() # 0을 패딩 토큰 ID로 사용\n",
        "\n",
        "def create_dataset(n_dates):\n",
        "    x, y = random_dates(n_dates)\n",
        "    return prepare_date_strs(x, INPUT_CHARS), prepare_date_strs(y, OUTPUT_CHARS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rcz_6S8dC46D"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "X_train, Y_train = create_dataset(10000)\n",
        "X_valid, Y_valid = create_dataset(2000)\n",
        "X_test, Y_test = create_dataset(2000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQiuiL8YC46D"
      },
      "outputs": [],
      "source": [
        "Y_train[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiZ7vky7C46D"
      },
      "source": [
        "### 첫 번째 버전: 기본적인 seq2seq 모델"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HtoNr-rC46D"
      },
      "source": [
        "먼저 가장 간단한 모델을 시도해 보겠습니다: 입력 시퀀스가 먼저 (임베딩 층 뒤에 하나의 LSTM 층으로 구성된) 인코더를 통과하여 벡터로 출력됩니다. 그 다음 이 벡터가 (하나의 LSTM 층 뒤에 밀집 층으로 구성된) 디코더로 들어가 벡터의 시퀀스를 출력합니다. 각 벡터는 가능한 모든 출력 문자에 대한 추정 확률입니다.\n",
        "\n",
        "디코더는 시퀀스를 입력으로 기대하기 때문에 가능한 가장 긴 출력 시퀀스만큼 (인코더의 출력) 벡터를 반복합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pjsQfMLvC46D"
      },
      "outputs": [],
      "source": [
        "embedding_size = 32\n",
        "max_output_length = Y_train.shape[1]\n",
        "\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "encoder = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=len(INPUT_CHARS) + 1,\n",
        "                           output_dim=embedding_size,\n",
        "                           input_shape=[None]),\n",
        "    tf.keras.layers.LSTM(128)\n",
        "])\n",
        "\n",
        "decoder = tf.keras.Sequential([\n",
        "    tf.keras.layers.LSTM(128, return_sequences=True),\n",
        "    tf.keras.layers.Dense(len(OUTPUT_CHARS) + 1, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    encoder,\n",
        "    tf.keras.layers.RepeatVector(max_output_length),\n",
        "    decoder\n",
        "])\n",
        "\n",
        "optimizer = tf.keras.optimizers.Nadam()\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
        "              metrics=[\"accuracy\"])\n",
        "history = model.fit(X_train, Y_train, epochs=20,\n",
        "                    validation_data=(X_valid, Y_valid))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJg19_aQC46D"
      },
      "source": [
        "좋아 보이네요, 100% 검증 정확도를 달성했습니다! 이 모델을 사용해 예측을 만들어 보죠. 문자 ID 시퀀스를 문자열로 바꾸는 함수를 작성하겠습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7gSr1tuoC46D"
      },
      "outputs": [],
      "source": [
        "def ids_to_date_strs(ids, chars=OUTPUT_CHARS):\n",
        "    return [\"\".join([(\"?\" + chars)[index] for index in sequence])\n",
        "            for sequence in ids]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ML55jOcC46D"
      },
      "source": [
        "이제 모델을 사용해 샘플 날짜를 변환합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AOqEWk75C46D"
      },
      "outputs": [],
      "source": [
        "X_new = prepare_date_strs([\"September 17, 2009\", \"July 14, 1789\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2w4nPudNC46D"
      },
      "outputs": [],
      "source": [
        "ids = model.predict(X_new).argmax(axis=-1)\n",
        "for date_str in ids_to_date_strs(ids):\n",
        "    print(date_str)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87pBdGs_C46E"
      },
      "source": [
        "완벽합니다! :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJGqPiEFC46E"
      },
      "source": [
        "하지만 (가장 긴 날짜에 해당하는) 길이가 18인 입력 문자열에서만 모델이 훈련되었기 때문에 짧은 시퀀스에서는 잘 동작하지 않습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nsCpmNEzC46E"
      },
      "outputs": [],
      "source": [
        "X_new = prepare_date_strs([\"May 02, 2020\", \"July 14, 1789\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yGpS56eC46E"
      },
      "outputs": [],
      "source": [
        "ids = model.predict(X_new).argmax(axis=-1)\n",
        "for date_str in ids_to_date_strs(ids):\n",
        "    print(date_str)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtsE9bM1C46E"
      },
      "source": [
        "이런! 패딩을 사용해 훈련할 때와 동일한 길이의 시퀀스를 전달해야 할 것 같습니다. 이를 위해 헬퍼 함수를 작성해 보죠:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ntqHdbRsC46E"
      },
      "outputs": [],
      "source": [
        "max_input_length = X_train.shape[1]\n",
        "\n",
        "def prepare_date_strs_padded(date_strs):\n",
        "    X = prepare_date_strs(date_strs)\n",
        "    if X.shape[1] < max_input_length:\n",
        "        X = tf.pad(X, [[0, 0], [0, max_input_length - X.shape[1]]])\n",
        "    return X\n",
        "\n",
        "def convert_date_strs(date_strs):\n",
        "    X = prepare_date_strs_padded(date_strs)\n",
        "    ids = model.predict(X).argmax(axis=-1)\n",
        "    return ids_to_date_strs(ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s40aAFFxC46E"
      },
      "outputs": [],
      "source": [
        "convert_date_strs([\"May 02, 2020\", \"July 14, 1789\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lLO0D0XC46E"
      },
      "source": [
        "좋네요! 물론 더 쉽게 날짜 변환 도구를 만들 수 있습니다(예를 들면, 정규식이나 더 단순한 문자열 조작). 하지만 신경망을 사용하는 것이 더 멋져 보이네요. ;-)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qg8viC-OC46E"
      },
      "source": [
        "하지만 실제 시퀀스-투-시퀀스 문제는 더 어렵습니다. 완벽함을 추구하기 위해 더 강력한 모델을 만들어 보겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixNvBf4eC46E"
      },
      "source": [
        "### 두 번째 버전: 디코더에서 한 타임 스텝 이동된 타깃 주입하기(티처 포싱(teacher forcing))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ftFtVskC46E"
      },
      "source": [
        "디코더에세 인코더 출력 벡터를 단순히 반복한 것을 주입하는 대신 한 타임 스텝 오른쪽으로 이동된 타깃 시퀀스를 주입할 수 있습니다. 이렇게 하면 각 타임 스텝에서 디코더는 이전 타깃 문자가 무엇인지 알게 됩니다. 이는 더 복잡한 시퀀스-투-시퀀스 문제를 다루는데 도움이 됩니다.\n",
        "\n",
        "각 타깃 시퀀스의 첫 번째 출력 문자는 이전 문자가 없기 때문에 시퀀스 시작(start-of-sequence, sos)을 나타내는 새로운 토큰이 필요합니다.\n",
        "\n",
        "추론에서는 타깃을 알지 못하므로 디코더에게 무엇을 주입해야 할까요? sos 토큰을 시작해서 한 번에 하나의 문자를 예측하고 디코더에게 지금까지 예측한 모든 문자를 주입할 수 있습니다(나중에 이 노트북에서 더 자세히 알아 보겠습니다).\n",
        "\n",
        "하지만 디코더의 LSTM이 스텝마다 이전 타깃을 입력으로 기대한다면 인코더의 벡터 출력을 어떻게 전달할까요? 한가지 방법은 출력 벡터를 무시하는 것입니다. 그리고 대신 인코더의 LSTM 상태를 디코더의 LSTM의 초기 상태로 사용합니다(이렇게 하려면 인코더의 LSTM과 디코더의 LSTM 유닛 개수가 같아야 합니다).\n",
        "\n",
        "그럼 (훈련, 검증, 테스트를 위한) 디코더의 입력을 만들어 보죠. sos 토큰은 가능한 출력 문자의 마지막 ID + 1으로 나타냅니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZFQ7sLYC46E"
      },
      "outputs": [],
      "source": [
        "sos_id = len(OUTPUT_CHARS) + 1\n",
        "\n",
        "def shifted_output_sequences(Y):\n",
        "    sos_tokens = tf.fill(dims=(len(Y), 1), value=sos_id)\n",
        "    return tf.concat([sos_tokens, Y[:, :-1]], axis=1)\n",
        "\n",
        "X_train_decoder = shifted_output_sequences(Y_train)\n",
        "X_valid_decoder = shifted_output_sequences(Y_valid)\n",
        "X_test_decoder = shifted_output_sequences(Y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jufA47OCC46E"
      },
      "source": [
        "디코더의 훈련 입력을 확인해 보죠:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "076Gm8L_C46F"
      },
      "outputs": [],
      "source": [
        "X_train_decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGK_DUv4C46F"
      },
      "source": [
        "이제 모델을 만듭니다. 이제 더 이상 간단한 시퀀셜 모델이 아니므로 함수형 API를 사용하겠습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Be1_R-AbC46F"
      },
      "outputs": [],
      "source": [
        "encoder_embedding_size = 32\n",
        "decoder_embedding_size = 32\n",
        "lstm_units = 128\n",
        "\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "encoder_input = tf.keras.layers.Input(shape=[None], dtype=tf.int32)\n",
        "encoder_embedding = tf.keras.layers.Embedding(\n",
        "    input_dim=len(INPUT_CHARS) + 1,\n",
        "    output_dim=encoder_embedding_size)(encoder_input)\n",
        "_, encoder_state_h, encoder_state_c = tf.keras.layers.LSTM(\n",
        "    lstm_units, return_state=True)(encoder_embedding)\n",
        "encoder_state = [encoder_state_h, encoder_state_c]\n",
        "\n",
        "decoder_input = tf.keras.layers.Input(shape=[None], dtype=tf.int32)\n",
        "decoder_embedding = tf.keras.layers.Embedding(\n",
        "    input_dim=len(OUTPUT_CHARS) + 2,\n",
        "    output_dim=decoder_embedding_size)(decoder_input)\n",
        "decoder_lstm_output = tf.keras.layers.LSTM(lstm_units, return_sequences=True)(\n",
        "    decoder_embedding, initial_state=encoder_state)\n",
        "decoder_output = tf.keras.layers.Dense(len(OUTPUT_CHARS) + 1,\n",
        "                                    activation=\"softmax\")(decoder_lstm_output)\n",
        "\n",
        "model = tf.keras.Model(inputs=[encoder_input, decoder_input],\n",
        "                           outputs=[decoder_output])\n",
        "\n",
        "optimizer = tf.keras.optimizers.Nadam()\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
        "              metrics=[\"accuracy\"])\n",
        "history = model.fit([X_train, X_train_decoder], Y_train, epochs=10,\n",
        "                    validation_data=([X_valid, X_valid_decoder], Y_valid))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71q6Co72C46F"
      },
      "source": [
        "이 모델도 100% 검증 정확도를 달성했지만 더 빠릅니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXTlJuN4C46F"
      },
      "source": [
        "이 모델을 사용해 몇 가지 예측을 수행해 보죠. 이번에는 한 문자씩 예측해야 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TWcxPDKEC46F"
      },
      "outputs": [],
      "source": [
        "sos_id = len(OUTPUT_CHARS) + 1\n",
        "\n",
        "def predict_date_strs(date_strs):\n",
        "    X = prepare_date_strs_padded(date_strs)\n",
        "    Y_pred = tf.fill(dims=(len(X), 1), value=sos_id)\n",
        "    for index in range(max_output_length):\n",
        "        pad_size = max_output_length - Y_pred.shape[1]\n",
        "        X_decoder = tf.pad(Y_pred, [[0, 0], [0, pad_size]])\n",
        "        Y_probas_next = model.predict([X, X_decoder])[:, index:index+1]\n",
        "        Y_pred_next = tf.argmax(Y_probas_next, axis=-1, output_type=tf.int32)\n",
        "        Y_pred = tf.concat([Y_pred, Y_pred_next], axis=1)\n",
        "    return ids_to_date_strs(Y_pred[:, 1:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIRhYhUpC46F"
      },
      "outputs": [],
      "source": [
        "predict_date_strs([\"July 14, 1789\", \"May 01, 2020\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwL2wFhkC46F"
      },
      "source": [
        "잘 동작하네요! 다음으로 트랜스포머 버전을 만들어 보세요. :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZKNAfcHC46F"
      },
      "source": [
        "## 10.\n",
        "_문제: 케라스 웹사이트에 있는 \"Natural language image search with a Dual Encoder\"(https://homl.info/dualtuto) 예제를 살펴보세요. 동일한 임베딩 공간 내에서 이미지와 텍스트를 모두 표현할 수 있는 모델을 만드는 방법을 배우게 됩니다. 이렇게 하면 OpenAI의 [CLIP 모델](https://openai.com/blog/clip/)에서와 같이 텍스트 프롬프트를 사용하여 이미지를 검색할 수 있습니다._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-If5_mvC46F"
      },
      "source": [
        "링크를 클릭하고 안내를 따르기만 하면 됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROiY3kzkC46F"
      },
      "source": [
        "## 11.\n",
        "_문제: 허깅 페이스의 트랜스포머스 라이브러리를 사용하여 텍스트를 생성할 수 있는 사전 훈련된 언어 모델(예, GPT)을 다운로드하고 보다 설득력 있는 셰익스피어식 텍스트를 생성해 보세요. 모델의 `generate()` 메서드를 사용해야 합니다. 자세한 내용은 허깅 페이스 온라인 문서를 참조하세요._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85Z2Aqq8C46F"
      },
      "source": [
        "먼저 사전 훈련된 모델을 로드해 보겠습니다. 이 예제에서는 추가 언어 모델(입력 임베딩에 가중치가 연결된 선형 층)을 위에 얹은 OpenAI의 GPT 모델을 사용합니다. 임포트하고 사전 훈련된 가중치를 로드해 보겠습니다(이렇게 하면 약 445MB의 데이터가 `~/.cache/torch/transformers`로 다운로드됩니다):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DfQngPaTC46G"
      },
      "outputs": [],
      "source": [
        "from transformers import TFOpenAIGPTLMHeadModel\n",
        "\n",
        "model = TFOpenAIGPTLMHeadModel.from_pretrained(\"openai-gpt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puEpHsoiC46G"
      },
      "source": [
        "다음으로 이 모델에 특화된 토크나이저가 필요합니다. 만약 설치되어 있으면 [spaCy](https://spacy.io/) 및 [ftfy](https://pypi.org/project/ftfy/) 라이브러리를 사용하려고 시도하고, 그렇지 않으면 BERT의 `BasicTokenizer`와 바이트 쌍 인코딩(대부분의 사용 사례에 적합할 것입니다)을 사용합니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_8yAG6rC46G"
      },
      "outputs": [],
      "source": [
        "from transformers import OpenAIGPTTokenizer\n",
        "\n",
        "tokenizer = OpenAIGPTTokenizer.from_pretrained(\"openai-gpt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlJNj6ufC46G"
      },
      "source": [
        "이제 토크나이저를 사용하여 프롬프트 텍스트를 토큰화 및 인코딩해 보겠습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fc3VlTQhC46G"
      },
      "outputs": [],
      "source": [
        "tokenizer(\"hello everyone\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJmW3sIcC46G"
      },
      "outputs": [],
      "source": [
        "prompt_text = \"This royal throne of kings, this sceptred isle\"\n",
        "encoded_prompt = tokenizer.encode(prompt_text,\n",
        "                                  add_special_tokens=False,\n",
        "                                  return_tensors=\"tf\")\n",
        "encoded_prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEjYrhZ_C46G"
      },
      "source": [
        "쉬워요! 다음으로 모델을 사용하여 프롬프트 뒤를 이은 텍스트를 생성해 보겠습니다. 프롬프트 텍스트로 시작하여 각각 5개의 다른 문장을 40개의 토큰 안에서 생성합니다. 모든 하이퍼파라미터의 기능에 대한 설명은 패트릭 폰 플라텐(Hugging Face)의 [블로그 게시물](https://huggingface.co/blog/how-to-generate)을 참조하세요. 하이퍼파라미터를 사용해 더 나은 결과를 얻을 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hp5KkqQ1C46G"
      },
      "outputs": [],
      "source": [
        "num_sequences = 5\n",
        "length = 40\n",
        "\n",
        "generated_sequences = model.generate(\n",
        "    input_ids=encoded_prompt,\n",
        "    do_sample=True,\n",
        "    max_length=length + len(encoded_prompt[0]),\n",
        "    temperature=1.0,\n",
        "    top_k=0,\n",
        "    top_p=0.9,\n",
        "    repetition_penalty=1.0,\n",
        "    num_return_sequences=num_sequences,\n",
        ")\n",
        "\n",
        "generated_sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMmMM7dEC46G"
      },
      "source": [
        "이제 생성된 시퀀스를 디코딩하고 인쇄해 보겠습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mC0KQG4UC46G"
      },
      "outputs": [],
      "source": [
        "for sequence in generated_sequences:\n",
        "    text = tokenizer.decode(sequence, clean_up_tokenization_spaces=True)\n",
        "    print(text)\n",
        "    print(\"-\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eqt-b4c2C46G"
      },
      "source": [
        "언어 모델이 위에 있는 변형을 포함하여 트랜스포머 라이브러리에서 사전 학습된 모델로 사용할 수 있는 GPT-2, CTRL, Transformer-XL 또는 XLNet과 같은 최신(및 더 큰) 모델을 사용해 볼 수 있습니다. 전처리 단계는 모델마다 조금씩 다르므로 트랜스포머 문서에서 이 [생성 예제](https://github.com/huggingface/transformers/blob/master/examples/run_generation.py)를 확인하시기 바랍니다(이 예제에서는 파이토치를 사용하지만 모델 클래스 이름 앞에 `TF`를 추가하고, `.to()` 메서드 호출을 제거하고, `\"pt\"` 대신 `return_tensors=\"tf\"`를 사용하는 등 약간의 조정만 하면 작동합니다)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "nav_menu": {},
    "toc": {
      "navigate_menu": true,
      "number_sections": true,
      "sideBar": true,
      "threshold": 6,
      "toc_cell": false,
      "toc_section_display": "block",
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}